---
title: "TAE - Unit 5 - Lecture 1"
author: "GT"
date: "May 12, 2016"
output: html_document
---

## Turning Tweets into Knowledge

We'll discuss how can use data from Twitter and turn them into knowledge.

- Twitter is a social networking and communication website founded in 2006.
- Users share and send messages that can be no longer than 140 characters long.
- It is indeed one of the Top 10 most visited sites on the internet.
- It had an initial public offering in 2013, and it's current valuation is about $31 billion.

Impact of Twitter

- Protesters around the world use Twitter. It had an important impact on Arab Spring.
- People also use it as a method of notification for natural disasters and for tracking diseases.
- Celebrities, politicians, and companies connect with fans and customers using Twitter.

Let us discuss a tweet that was supposed to be from the Associated Press.
The Associated Press is a major news agency that distributes news stories to other news 
agencies.

In April 2013, someone tweeted this message from the main AP verified Twitter account.
*Breaking: Two explosions in the White House and Barack Obama is injured.*

The S&P 500 stock index fell 1% in seconds, but the White House rapidly clarified.

Many companies maintain online presences. Managing public perception in an age of 
instant communication is essential. Reacting to changing sentiment, identifying offensive
posts, determining topics of interest are indeed crucial.

### Text Analytics

Until now, we have seen data that are structured, numerical, or categorical.

On the other hand, tweets are loosely structured. They are often textual.
They have poor spelling, often contain non-traditional grammar, and they are multilingual.

In this example here, we see two examples of this aspect of tweets.

*MY ELECTRIC HAS WENT OUT AND A GIANT SPIDER IS COMING 4  ME ANd mY ONLY* 
*SOURCE OF LIGHT IS THE FLASHLIGHT ON MAY PHONE GOD BLESS @Apple*

*WHYCANT I GO BACK TO IOS6 ITS NOT THAT BIG A DEAL @Apple I LIKE YOUR OLD* 
*OPERATING SYSTEM BETTER*

A key question is how to handle this information included in the tweets.

Humans cannot, of course, keep up with internet-scale volumes of data as there are
about half a billion tweets per day. Even at the small scale, the cost and time
required to process this is of course prohibitive.

How can computers help?

The field that addresses how computers understand text is called 
*Natural Language Processing.*

The goal is to understand and derive meaning from human language.

In 1950, Alan Turing, a major computer scientist of the era, proposed a test of machine 
intelligence. That the computer program passes it if it can take part in a real-time 
conversation and cannot be distinguished from a human.

Let's discuss briefly the history of Natural Language Processing.

There has been some progress, for example, the "chatterbot" ELIZA.

The initial focus has been on understanding grammar. Later, the focus shifted towards
statistical, machine learning techniques that learn from large bodies of text.

Today, there are modern versions of these Natural Language Processing.

Apple is using Siri, and Google is using Google Now.

Why is it hard?

Let us give you an example.

Suppose we say the phrase: "I put my *bag* in the *car*. *It* is *large and blue*"

The question is, does the "it" refer to the bag or the "it" refers to car?

The context is often important. Humans use homonyms, metaphors, often sarcasm.

### Creating the Dataset

In this lecture, we'll be trying to understand sentiment of tweets about the company Apple.

Apple is a computer company known for its laptops, phones, tablets, and personal media players.
While Apple has a large number of fans, they also have a large number of people who don't like 
their products. And they have several competitors. 

To better understand public perception, Apple wants to monitor how people feel over time
and how people receive new announcements.

Our challenge in this lecture is to see if we can correctly classify tweets as being negative, 
positive, or neither about Apple.

To collect the data needed for this task, we had to perform two steps.

The first steps was to collect data about tweets from the internet. Twitter data is 
publicly available.   And you can collect it through scraping the website or by using a
special interface for programmers   that Twitter provides called an API.
The sender of the tweet might be useful to predict sentiment. But we'll ignore it to keep 
our data anonymized.

So we'll just be using the text of the tweet.

Then we need to construct the outcome variable for these tweets, which means that we 
have to label them as positive, negative, or neutral sentiment. 

We would like to label thousands of tweets. And we know that two people might disagree over
the correct classification of a tweet. 

So to do this efficiently, one option is to use the **Amazon Mechanical Turk**.

So what is the *Amazon Mechanical Turk*?

It allows people to break tasks down into small components and then enables them to 
distribute these tasks online to be solved by people all over the world.
People can sign up to perform the available tasks for a fee. For example, we might pay $0.02 
for a single classified tweet. The Amazon Mechanical Turk serves as a broker and takes
a small cut of the money.

Many of the tasks on the Mechanical Turk require human intelligence, like classifying
the sentiment of a tweet. But these tasks may be time consuming or require building otherwise 
unneeded capacity for the creator of the task.And so it's appealing to outsource the job.

The task that we put on the Amazon Mechanical Turk was to 

*judge the sentiment expressed by the following item toward the software company "Apple"*

The items we gave them were tweets that we had collected. The workers could pick from the
following options:

- strongly negative (-2)
- negative (-1)
- neutral (0)
- positive (+1)
- strongly positive (+2)

```
^
|               .------.
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|        .-----.|      |
|        |     ||      |
|        |     ||  912 |.-----.
| .-----.| 317 ||      ||     |.-----.
| | 51  ||     ||      || 140 || 17  |
--'-----''-----''------''-----''-----'->
   -2.0   -1.0     0      1.0    2.0 
```

We had five workers label each tweet.

The graph on the right shows the distribution of the number of tweets classified
into each of the categories. 

We can see here that the majority of tweets were classified as neutral, with a small 
number classified as strongly negative or strongly positive.

Then, for each tweet, we take the average of the five scores given by the five workers.

For example, 

- *"LOVE U @APPLE"* (1.8), was seen as strongly positive by 4 of the workers
  and positive by one of the workers.
- *"@ apple  @twitter Happy Programmers' Day folks!"* (0.4) was seen as slightly positive on average.
- *"So disappointed in @Apple. Sold me a Macbook Air that WONT run my apps.*
  *So I have to drive hours to return it. They won't let me ship it."* (-1.4) was seen as 
  pretty negative.
  
So now we have a bunch of tweets that are labeled with their sentiment.

But how do we build independent variables from the text of a tweet to be used to predict 
the sentiment?

### Bag of Words

In this lecture, we'll use a technique called **Bag of Words** to build text analytics models.

Fully understanding text is difficult, but Bag of Words provides a very simple approach.

It just **counts the number of times** each word appears in the text and uses these counts
as the independent variables.

For example, in the sentence, 

*"This course is great.I would recommend this course my friends,"*

```
-------------------------------------------------
| THIS | COURSE | GREAT | ... | WOULD | FRIENDS |
-------------------------------------------------
|  2   |   2    |   1   | ... |   1   |    1    |
-------------------------------------------------
```

In Bag of Words, there's one feature for each word. This is a very simple approach, 
but is often very effective, too.

It's used as a baseline in text analytics projects and for natural language processing.

This isn't the whole story, though. Preprocessing the text can dramatically improve the 
performance of the Bag of Words method. One part of preprocessing the text is to clean up 
irregularities.

Text data often as many inconsistencies that will cause algorithms trouble.
Computers are very literal by default.

'Apple' with just an uppercase A, 'APPLE' all in uppercase letters, or 
'ApPLe' with a mixture of uppercase and lowercase letters will all be counted separately.
 
We want to change the text so that all three versions of Apple here will be counted as the same 
word, by either changing all words to uppercase or to lower case. We'll typically change all 
the letters to lowercase.

Punctuation can also cause problems. The basic approach is to deal with this is to remove 
everything that isn't a standard number or letter. 

However, sometimes punctuation is meaningful.

- In the case of Twitter, `@Apple` denotes a message to Apple, and `#Apple` is a message about Apple.
- For web addresses, the punctuation often defines the web address.

For these reasons, the removal of punctuation should be tailored to the specific problem.

In our case, we will remove all punctuation, so @Apple, Apple with an exclamation point,
Apple with dashes will all count as just Apple.

Another preprocessing task we want to do is to remove unhelpful terms.

Many words are frequently used but are only meaningful in a sentence.

These are called **stop words**. Examples are *the*, *is*, *at*, and *which*.
It's unlikely that these words will improve the machine learning prediction quality,
so we want to remove them to reduce the size of the data.

There are some potential problems with this approach. Sometimes, two stop words taken 
together have a very important meaning.

It no longer has the same meaning as before. So while removing stop words sometimes 
is not helpful, it generally is a very helpful preprocessing step.

Lastly, an important preprocessing step is called **stemming**.

This step is motivated by the desire to represent words with different endings
as the same word.

We probably do not need to draw a distinction between argue, argued, argues, and arguing.

They could all be represented by a common *stem*, **argu**.

There are many ways to approach the problem. One approach is to build a database
of words and their stems. A pro is that this approach handles exceptions very nicely,
since we have defined all of the stems.

However, it won't handle new words at all, since they are not in the database.
This is especially bad for problems where we're using data from the internet,
since we have no idea what words will be used.

A different approach is to write a rule-based algorithm. In this approach, if a word ends
in things like *ed*, *ing*, or *ly*, we would remove the ending. A pro of this approach
is that it handles new or unknown words well.

However, there are many exceptions, and this approach would miss all of these.

Words like child and children would be considered different, but it would get other plurals, 
like dog and dogs.

This second approach is widely popular and is called the Porter Stemmer, designed
by Martin Porter in 1980, and it's still used today.

Stemmers like this one have been written for many languages. Other options for stemming 
include machine learning, where algorithms are trained to recognize the roots of words and
combinations of the approaches explained here.

As a real example from our data set, the phrase *"by far the best customer care service I*
*have ever received"* has three words that would be stemmed: 'customer', 'service', and 'received'.

- the "er" would be removed in customer
- the "e" would be removed in service
- the "ed" would be removed in received

### Pre-processing in R

Pre-processing the data can be difficult, but, luckily, R's packages provide easy-to-use
functions for the most common tasks.

In your R console, let's load the data set tweets.csv with the read.csv function.
But since we're working with text data here, we need one extra argument, which is
stringsAsFactors=FALSE.

```{r}
tweets = read.csv("tweets.csv", stringsAsFactors = FALSE)
str(tweets)
```

We can see that we have 1,181 observations of two variables, the text of the tweet, 
called *Tweet*, and the average sentiment score, called *Avg* for average.

The tweet texts are real tweets that we found on the internet directed to Apple with
a few cleaned up words.

We're more interested in being able to detect the tweets with clear negative sentiment,
so let's define a new variable in our data set tweets called Negative. And we'll set this 
equal to as.factor(tweets$Avg <= -1). This will set tweets$Negative equal to true if the 
average sentiment score is less than or equal to negative 1 and will set tweets$Negative
equal to false if the average sentiment score is greater than negative 1.

```{r}
tweets$Negative = as.factor(tweets$Avg <= -1)
table(tweets$Negative)
```

We can see that 182 of the 1,181 tweets, or about 15%, are negative.

Now to pre-process our text data so that we can use the bag of words approach,
we'll be using the tm text mining package. Then we also need to install the package SnowballC.
This package helps us use the tm package.

```{r}
library(tm)
library(SnowballC)
Sys.setlocale("LC_ALL", "C")
```

One of the concepts introduced by the tm package is that of a corpus.
A corpus is a collection of documents.

We'll need to convert our tweets to a corpus for pre-processing. tm can create a corpus 
in many different ways, but we'll create it from the tweet column of our data frame
using two functions, *Corpus* and *VectorSource*.

```{r}
corpus = Corpus(VectorSource(tweets$Tweet))
corpus
as.character(corpus[[1]])
```


Now we're ready to start pre-processing our data. Pre-processing is easy in tm.
Each operation, like stemming or removing stop words, can be done with one line in R, where
we use the tm_map function. 

Let's try it out by changing all of the text in our tweets to lowercase.

```{r}
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument) # converts all documents in the corpus to PlainTextDocument type
as.character(corpus[[1]])
```

Now let's remove all punctuation. This is done in a very similar way,
except this time we give the argument removePunctuation instead of tolower.

```{r}
corpus = tm_map(corpus, removePunctuation)
as.character(corpus[[1]])
```

Now we want to remove the stop words in our tweets. tm provides a list of stop words for 
the English language. 

```{r}
stopwords("english")[1:10]
length(stopwords("english"))
```

Removing words can be done with the removeWords argument to the tm_map function, 
but we need one extra argument, what the stop words are that we want to remove.
We'll remove all of these English stop words, but we'll also remove the word "apple"
since all of these tweets have the word "apple" and it probably won't be very useful in 
our prediction problem.

```{r}
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
as.character(corpus[[1]])
```

Lastly, we want to stem our document with the stemDocument argument.

```{r}
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
```

We can see that this took off the ending of "customer," "service," "received," and "appstore."

### Bag of Words in R

We're now ready to extract the word frequencies to be used in our prediction problem.

The tm package provides a function called DocumentTermMatrix that generates a matrix 
where the rows correspond to documents, in our case tweets, and the columns correspond 
to words in those tweets. The values in the matrix are the number of times that word 
appears in each document.

Let's go ahead and generate this matrix and call it "frequencies."

```{r}
frequencies = DocumentTermMatrix(corpus)
frequencies
```

We can see that there are 3,289 terms or words in our matrix and 1,181 documents
or tweets after preprocessing.

Let's see what this matrix looks like using the inspect function.

```{r}
inspect(frequencies[1000:1005,505:515])
```

In this range we see that the word "cheer" appears in the tweet 1005, but "cheap" doesn't
appear in any of these tweets. This data is what we call *sparse*.

This means that there are many zeros in our matrix. 

We can look at what the most popular terms are, or words, with the function findFreqTerms.

```{r}
findFreqTerms(frequencies, lowfreq = 20) # lowFeq is equal to the minimum number of times a term must be displayed
```

We see here 56 different words. So out of the 3,289 words in our matrix,
only 56 words appear at least 20 times in our tweets. This means that we probably have
a lot of terms that will be pretty useless for our prediction model.

The number of terms is an issue for two main reasons.

One is computational. More terms means more independent variables, which usually means
it takes longer to build our models.

The other is in building models, as we mentioned before, the ratio of independent 
variables to observations will affect how good the model will generalize.

So let's remove some terms that don't appear very often. 
The sparsity threshold works as follows. If we say 0.98, this means to only keep
terms that appear in 2% or more of the tweets. If we say 0.99, that means to only keep
terms that appear in 1% or more of the tweets.

```{r}
sparse = removeSparseTerms(frequencies, 0.995) # terms that appearin 0.5% or more of the tweets
sparse
```

There's only 309 terms in our sparse matrix. This is only about 9% of the previous 
count of 3,289.

Now let's convert the sparse matrix into a data frame that we'll be able to use 
for our predictive models.

```{r}
tweetsSparse = as.data.frame(as.matrix(sparse))
```

Since R struggles with variable names that start with a number, and we probably have
some words here that start with a number, let's run the make.names function to make sure
all of our words are appropriate variable names.

```{r}
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
```

This will just convert our variable names to make sure they're all appropriate names
before we build our predictive models. You should do this each time you've
built a data frame using text analytics.

Now let's add our dependent variable to this data set. We'll call it tweetsSparse$Negative
and set it equal to the original Negative variable from the tweets data frame.

```{r}
tweetsSparse$Negative = tweets$Negative
```

Lastly, let's split our data into a training set and a testing set, putting 70% of 
the data in the training set.

```{r}
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweetsSparse, split == TRUE)
testSparse = subset(tweetsSparse, split == FALSE)
```

Our data is now ready, and we can build our predictive model.

### Predicting Sentiment

Now that we've prepared our data set, let's use CART to build a predictive model. 
We'll use the rpart function to predict Negative using all of the other variables
as our independent variables and the data set trainSparse.

```{r}
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data=trainSparse, method="class") # to build a classification model
# we're just using the default parameter setting so we won't add anything for minbucket or cp
prp(tweetCART)
```

Our tree says that if the word "freak" is in the tweet, then predict TRUE, or negative sentiment.
If the word "freak" is not in the tweet, but the word "hate" is, again predict TRUE.
If neither of these two words are in the tweet, but the word "wtf" is, also predict TRUE,
or negative sentiment. If none of these three words are in the tweet, then predict FALSE,
or non-negative sentiment.

This tree makes sense intuitively since these three words are generally seen as negative words.

Now, we'll evaluate the numerical performance of our model by making predictions on the test set.

```{r}
predictCART = predict(tweetCART, newdata = testSparse, type="class")
table(testSparse$Negative, predictCART)
# confusion matrix
(294+18)/(294+6+37+18) # accuracy
```

So the accuracy of our CART model is about 0.88.

Let's compare this to a simple baseline model that always predicts non-negative.

To compute the accuracy of the baseline model, let's make a table of just the outcome 
variable Negative.

```{r}
table(testSparse$Negative)
300/(300+355)
```

So the accuracy of a baseline model that always predicts non-negative is 0.845.

So our CART model does better than the simple baseline model.

How about a random forest model? How well would that do?

Let's first load the random forest package with library(randomForest), and then we'll
set the seed to 123 so that we can replicate our model if we want to.

```{r}
library(randomForest)
set.seed(123)
```

Now, let's create our model.

```{r}
tweetRF = randomForest(Negative ~ ., data=trainSparse)
# The random forest model takes significantly longer to build than the CART model.
predictRF = predict(tweetRF, newdata = testSparse)
# confusion matrix
table(testSparse$Negative, predictRF)
(293+21)/(293+7+34+21) # accuracy
```

This is a little better than our CART model, but due to the interpretability of our CART model,
I'd probably prefer it over the random forest model. If you were to use cross-validation
to pick the cp parameter for the CART model, the accuracy would increase to about the same 
as the random forest model.

So by using a bag-of-words approach and these models, we can reasonably predict sentiment even
with a relatively small data set of tweets.

### Conclusions


In this lecture, we have seen a particular application of sentiment analysis on Twitter.

However, the area of sentiment analysis is much broader. Over 7,000 research articles 
have been written on the topic. Hundreds of start-ups are developing sentiment analysis
solutions. Many websites perform real-time analysis of tweets. 

For example, "tweetfeel" shows trends given any term, and "The Stock Sonar"
shows sentiment and stock prices.

Sentiment analysis is a particular application of text analytics. In general,
the critical aspect of text analytics is to select the specific features that
are relevant in a particular application.

In addition, it's important to apply specific knowledge that often leads to better results.

For example, using the meaning of the symbols or include features like the number of words.

Let's finally discuss the analytics edge that we have seen in this lecture.

Analytical sentiment analysis we have seen can replace more labor-intensive methods like polling.
Text analytics can also deal with the massive amounts of unstructured data being generated on the
internet. Computers are becoming more and more capable of interacting with humans
and performing human tasks.
