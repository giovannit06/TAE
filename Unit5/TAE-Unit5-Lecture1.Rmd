---
title: "TAE - Unit 5 - Lecture 1"
author: "GT"
date: "May 12, 2016"
output: html_document
---

## Turning Tweets into Knowledge

We'll discuss how can use data from Twitter and turn them into knowledge.

- Twitter is a social networking and communication website founded in 2006.
- Users share and send messages that can be no longer than 140 characters long.
- It is indeed one of the Top 10 most visited sites on the internet.
- It had an initial public offering in 2013, and it's current valuation is about $31 billion.

Impact of Twitter

- Protesters around the world use Twitter. It had an important impact on Arab Spring.
- People also use it as a method of notification for natural disasters and for tracking diseases.
- Celebrities, politicians, and companies connect with fans and customers using Twitter.

Let us discuss a tweet that was supposed to be from the Associated Press.
The Associated Press is a major news agency that distributes news stories to other news 
agencies.

In April 2013, someone tweeted this message from the main AP verified Twitter account.
*Breaking: Two explosions in the White House and Barack Obama is injured.*

The S&P 500 stock index fell 1% in seconds, but the White House rapidly clarified.

Many companies maintain online presences. Managing public perception in an age of 
instant communication is essential. Reacting to changing sentiment, identifying offensive
posts, determining topics of interest are indeed crucial.

### Text Analytics

Until now, we have seen data that are structured, numerical, or categorical.

On the other hand, tweets are loosely structured. They are often textual.
They have poor spelling, often contain non-traditional grammar, and they are multilingual.

In this example here, we see two examples of this aspect of tweets.

*MY ELECTRIC HAS WENT OUT AND A GIANT SPIDER IS COMING 4  ME ANd mY ONLY* 
*SOURCE OF LIGHT IS THE FLASHLIGHT ON MAY PHONE GOD BLESS @Apple*

*WHYCANT I GO BACK TO IOS6 ITS NOT THAT BIG A DEAL @Apple I LIKE YOUR OLD* 
*OPERATING SYSTEM BETTER*

A key question is how to handle this information included in the tweets.

Humans cannot, of course, keep up with internet-scale volumes of data as there are
about half a billion tweets per day. Even at the small scale, the cost and time
required to process this is of course prohibitive.

How can computers help?

The field that addresses how computers understand text is called 
*Natural Language Processing.*

The goal is to understand and derive meaning from human language.

In 1950, Alan Turing, a major computer scientist of the era, proposed a test of machine 
intelligence. That the computer program passes it if it can take part in a real-time 
conversation and cannot be distinguished from a human.

Let's discuss briefly the history of Natural Language Processing.

There has been some progress, for example, the "chatterbot" ELIZA.

The initial focus has been on understanding grammar. Later, the focus shifted towards
statistical, machine learning techniques that learn from large bodies of text.

Today, there are modern versions of these Natural Language Processing.

Apple is using Siri, and Google is using Google Now.

Why is it hard?

Let us give you an example.

Suppose we say the phrase: "I put my *bag* in the *car*. *It* is *large and blue*"

The question is, does the "it" refer to the bag or the "it" refers to car?

The context is often important. Humans use homonyms, metaphors, often sarcasm.

### Creating the Dataset

In this lecture, we'll be trying to understand sentiment of tweets about the company Apple.

Apple is a computer company known for its laptops, phones, tablets, and personal media players.
While Apple has a large number of fans, they also have a large number of people who don't like 
their products. And they have several competitors. 

To better understand public perception, Apple wants to monitor how people feel over time
and how people receive new announcements.

Our challenge in this lecture is to see if we can correctly classify tweets as being negative, 
positive, or neither about Apple.

To collect the data needed for this task, we had to perform two steps.

The first steps was to collect data about tweets from the internet. Twitter data is 
publicly available.   And you can collect it through scraping the website or by using a
special interface for programmers   that Twitter provides called an API.
The sender of the tweet might be useful to predict sentiment. But we'll ignore it to keep 
our data anonymized.

So we'll just be using the text of the tweet.

Then we need to construct the outcome variable for these tweets, which means that we 
have to label them as positive, negative, or neutral sentiment. 

We would like to label thousands of tweets. And we know that two people might disagree over
the correct classification of a tweet. 

So to do this efficiently, one option is to use the **Amazon Mechanical Turk**.

So what is the *Amazon Mechanical Turk*?

It allows people to break tasks down into small components and then enables them to 
distribute these tasks online to be solved by people all over the world.
People can sign up to perform the available tasks for a fee. For example, we might pay $0.02 
for a single classified tweet. The Amazon Mechanical Turk serves as a broker and takes
a small cut of the money.

Many of the tasks on the Mechanical Turk require human intelligence, like classifying
the sentiment of a tweet. But these tasks may be time consuming or require building otherwise 
unneeded capacity for the creator of the task.And so it's appealing to outsource the job.

The task that we put on the Amazon Mechanical Turk was to 

*judge the sentiment expressed by the following item toward the software company "Apple"*

The items we gave them were tweets that we had collected. The workers could pick from the
following options:

- strongly negative (-2)
- negative (-1)
- neutral (0)
- positive (+1)
- strongly positive (+2)

```
^
|               .------.
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|               |      |
|        .-----.|      |
|        |     ||      |
|        |     ||  912 |.-----.
| .-----.| 317 ||      ||     |.-----.
| | 51  ||     ||      || 140 || 17  |
--'-----''-----''------''-----''-----'->
   -2.0   -1.0     0      1.0    2.0 
```

We had five workers label each tweet.

The graph on the right shows the distribution of the number of tweets classified
into each of the categories. 

We can see here that the majority of tweets were classified as neutral, with a small 
number classified as strongly negative or strongly positive.

Then, for each tweet, we take the average of the five scores given by the five workers.

For example, 

- *"LOVE U @APPLE"* (1.8), was seen as strongly positive by 4 of the workers
  and positive by one of the workers.
- *"@ apple  @twitter Happy Programmers' Day folks!"* (0.4) was seen as slightly positive on average.
- *"So disappointed in @Apple. Sold me a Macbook Air that WONT run my apps.*
  *So I have to drive hours to return it. They won't let me ship it."* (-1.4) was seen as 
  pretty negative.
  
So now we have a bunch of tweets that are labeled with their sentiment.

But how do we build independent variables from the text of a tweet to be used to predict 
the sentiment?

### Bag of Words

In this lecture, we'll use a technique called Bag of Words
to build text analytics models.
Fully understanding text is difficult, but Bag of Words
provides a very simple approach.
It just counts the number of times
each word appears in the text and uses these counts
as the independent variables.
For example, in the sentence, "This course is great.
I would recommend this course my friends,"
the word this is seen twice, the word course is seen twice,
the word great is seen once, et cetera.
In Bag of Words, there's one feature for each word.
This is a very simple approach, but is often very effective,
too.
It's used as a baseline in text analytics projects
and for natural language processing.
This isn't the whole story, though.
Preprocessing the text can dramatically
improve the performance of the Bag of Words method.
One part of preprocessing the text
is to clean up irregularities.
Text data often as many inconsistencies that will cause
algorithms trouble.
Computers are very literal by default.
Apple with just an uppercase A, APPLE all in uppercase letters,
or ApPLe with a mixture of uppercase and lowercase letters
will all be counted separately.
We want to change the text so that all three
versions of Apple here will be counted as the same word,
by either changing all words to uppercase or to lower case.
We'll typically change all the letters to lowercase,
so these three versions of Apple will all
become Apple with lower case letters
and will be counted as the same word.
Punctuation can also cause problems.
The basic approach is to deal with this
is to remove everything that isn't a standard number
or letter.
However, sometimes punctuation is meaningful.
In the case of Twitter, @Apple denotes a message to Apple,
and #Apple is a message about Apple.
For web addresses, the punctuation
often defines the web address.
For these reasons, the removal of punctuation
should be tailored to the specific problem.
In our case, we will remove all punctuation, so @Apple,
Apple with an exclamation point, Apple with dashes
will all count as just Apple.
Another preprocessing task we want to do
is to remove unhelpful terms.
Many words are frequently used but are
only meaningful in a sentence.
These are called stop words.
Examples are the, is, at, and which.
It's unlikely that these words will improve
the machine learning prediction quality,
so we want to remove them to reduce the size of the data.
There are some potential problems with this approach.
Sometimes, two stop words taken together
have a very important meaning.
For example, "The Who"-- which is a combination of two stop
words-- is actually the name of the band we see on the right
here.
By removing the stop words, we remove both of these words,
but The Who might actually have a significant meaning
for our prediction task.
Another example is the phrase, "Take That".
If we remove the stop words, we'll
remove the word "that," so the phrase would just say, "take."
It no longer has the same meaning as before.
So while removing stop words sometimes is not helpful,
it generally is a very helpful preprocessing step.
Lastly, an important preprocessing step
is called stemming.
This step is motivated by the desire
to represent words with different endings
as the same word.
We probably do not need to draw a distinction between argue,
argued, argues, and arguing.
They could all be represented by a common stem, argu.
The algorithmic process of performing this reduction
is called stemming.
There are many ways to approach the problem.
One approach is to build a database
of words and their stems.
A pro is that this approach handles exceptions very nicely,
since we have defined all of the stems.
However, it won't handle new words at all,
since they are not in the database.
This is especially bad for problems
where we're using data from the internet,
since we have no idea what words will be used.
A different approach is to write a rule-based algorithm.
In this approach, if a word ends in things like ed, ing, or ly,
we would remove the ending.
A pro of this approach is that it handles new or unknown words
well.
However, there are many exceptions,
and this approach would miss all of these.
Words like child and children would be considered different,
but it would get other plurals, like dog and dogs.
This second approach is widely popular
and is called the Porter Stemmer, designed
by Martin Porter in 1980, and it's still used today.
Stemmers like this one have been written for many languages.
Other options for stemming include
machine learning, where algorithms
are trained to recognize the roots of words and combinations
of the approaches explained here.
As a real example from our data set,
the phrase "by far the best customer care service I
have ever received" has three words
that would be stemmed-- customer, service,
and received.
The "er" would be removed in customer,
the "e" would be removed in service,
and the "ed" would be removed in received.
In the next video, we'll see how to run these preprocessing
steps in R.