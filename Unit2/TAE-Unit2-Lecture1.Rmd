---
title: "TAE - Unit 2 - Lecture 1"
author: "GT"
date: "19 avril 2016"
output: html_document
---

## Unit 2 - Lecture 1 - The Statistical Sommelier

In this lecture, the linear regression is introduced. A simple but very powerful method to analyze data and
make predictions and apply it here in a very unexpected context - predicting the quality of wines.

### One-variable - Linear Regression

This method just uses one independent variable to predict the dependent variable. The goal of linear regression
is to create a predictive line through the data.

In general form a one-variable linear regression model is a linear equation to predict the dependent variable,
y, using the independent variable, x.

$y^i = \beta_0 + \beta_{1}x^i + \epsilon^i$

- $\beta_0$ is the intercept term or intercept coefficient
- $\beta_1$ is the slope of the line or coefficient for the independent variable, x.

For each observation, $i$, there are data for the dependent variable $y^i$ and data for the independent
variable $x^i$. Using this equation it's possible to make a prediction for each data point, $i$. 
Since the coefficients have to be the same for all data points, the small error made it's called $\epsilon^i$, 
called often a *redidual*.
The best mode or best choice of coefficients has the smallest error terms or smallest residuals.

It's possible to compute the residual of the line for each data point. One measure of the quality of a
regression line is the sum of squared errors, or *SSE*. This is the sum of the squared residuals.

$SSE = (\epsilon^1)^2+(\epsilon^2)^2+...+(\epsilon^N)^2$

$N$ = number of data points

Although $SSE$ value allows us to compare lines on the same data set, it's hard to interpret for two reason.

1. It scales with $N$, the number of data points. The same model can have twice as much data then the $SSE$
can be twice as big.
2. The units are hard to understand.

Because of these problems, *Root Means Squared Error*, or *RMSE* is often used.

$RMSE = \sqrt{\frac{SSE}{N}}$

This is normalized by n and is in the same units as the dependend variable.

Another common error measure for linear regression is $R^2$. This error measure compares the best model to
a baseline mode, the model that does not use any variables. The baseline model predicts the average value of 
the dependent variable regardless of the value of the independent variable.

The sum of squared errors for the baseline model is also known as the *Total Sum of Squares* or *SST*.

$R^2 = 1-\frac{SSE}{SST}$

R squared captures the value added from using a linear regression model over just predicting the average
outcome for every data point. 

What value do we expect to see for $R^2$?

- The linear regression model will never be worse than the baseline model --> $SSE$ = $SST$ --> $R^2=0$
- In the best case the linear regression model makes no error --> $SSE = 0$ --> $R^2=1$

So an $R^2$ equal or close to 1 means a perfect or almost perfect predictive model.

### Multiple Linear Regression

It's possible to use more than one variable to predict values. For example, we can use each variable
in a one-variable regression model and choose the best one, that gives higher values for $R^2$. But
multiple linear regression allows you to use multiple variables at once to improve the model.

The multiple linear regression model is similar to the one variable regression model that has a coefficient
beta for each independent variables.

$y^i=\beta_0+\beta_1x_1^i+\beta_2x_2^i+...+\beta_kx_k^i+\epsilon^i$

The dependent variable, $y$, is predicted using the independent vaniables, $x_1$, $x_2$, through, $x_k$, 
where $k$ denotes the number of independent variables. The best model is selected in the same way as before.
To minimize the sum of squared errors, using the error term, $\epsilon$.

We can start by building a linear regression model that just uses the variable with the best $R^2$. Then we can
add variables one at a time and look at improvement in $R^2$. The improvement is not equal for eache variables
add since they're interactions between the independent variables.

So with model should be used? Often not all variables should be used. Because this creates a more 
complicated model and this often cause an *overfitting*.

### Linear Regression in R

Load the dataset. Look at the structure of the data using the str() function and at the statistical
summary of the data using the summary() function.

```{r}
wine = read.csv("wine.csv")
str(wine)
summary(wine)
```

Create now a one-variable linear regression equation using AGST to predict Price. The regression
model will be called `model1` and we'll use the lm() fuction, which stands for linear model.

```{r}
model1 = lm(Price ~ AGST, data=wine) # First argument: dependent variable ~ independent variable, Second argument: dataset used to build the model
```

Take a look at the summary of model1.

```{r}
summary(model1)
```

First thing we see is a description of the function we used to build the model, then a summaary of the
residual or error terms. Following that is a description of the coefficients of the model. The first row
corresponds to the intercept term, and the second row corresopnds to the independent variable, AGST.
The Estimat column gives estimates of the beta values for the model. Towards the bottom of the output, you
can see Multiple R-squared, 0.435, which is the R-squared value discussed before. Beside it is a number
labeled Adjusted R-squared, 0.41. This number take into account the number of independent variables used
relative to the number of data points. Multiple R-squared will always increase if you add more independent
variables. But Adjusted R-squared will decrease if you add independent variable that doesn't help the model.

Compute the sum of squared errors, or SSE. The residuals are stored in the vector `model1$rediduals`.

```{r}
model1$residuals
SSE = sum(model1$residuals^2)
SSE
```

Add now another variable to the regression model, `HarvestRain` and call the new model `model2`. Then take
a look at the summary.

```{r}
model2 = lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
```

There is a third row in the Coefficient table now corresponding to HarvestRain. This coefficient is negative
0.00457. And if you look at the R-squared value, you can see that this variable really helped the model.

Compute now the sum of squared errors for this new model.

```{r}
SSE = sum(model2$residuals^2)
SSE
```

Build now a third model, `model3`, with all the independent variables and look at the summary.

```{r}
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
```

If we look at the bottom of the output, we can again see that the Multiple R-squared and Adjusted R-squared
have both increased.

Compute the sum of squared errors, SSE.

```{r}
SSE = sum(model3$residuals^2)
SSE
```

See that the SSE is better than before,

### Understanding the Model

In the Coefficients section, the independent variables are listed on the left.
The Estimates column gives the coefficients for the intercept and for each
of the independent variables in the model.

The remaining columns help us to determine if a variable should be included in 
the model, or if the coefficient is significally different from 0.

A coefficient of 0 means that the value of the independent variable does not 
change the prediction for the dependent variable and if it's not significantly
different from 0, then it should probably removed from the model.

The Standard error column gives us a measure of how much the coefficient is
likely to vary from the estimate value. 

The t value is the Estimate divided by the Standard error. The larger the absolute
value of the t value, the more likely the coefficient is to be significant.

The last column gives a measure of how plausible it is that the coefficient is 
actally 0. The less plausible it is (smaller probability), the less likely it is
that the coefficient estimate is actually 0.

We want independent variables with small values in this column. The easiest way 
to know is to look at the stars at the end of each row. 

- `***` is the highest level of significance (p < 0.001).
- `**` is also very significant (p < 0.01)
- `*` is still significant (p < 0.05)
- `.` is almost significant (p < 0.1)
- `' '` means that the variable is not significant (p > 0.1)

In the summary(model3), we can see that the variables Age and FrancePopulation
are insignificant in the model. 
Let's start by removing FrancePopulation from the model.

```{r}
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
summary(model4)
```

In this model the Multiple R-squared and the Adjusted R-squared are similar to
the previous one. That means that the model is just as strong then the previous
model.

If we look now at each of the independent variables and the stars, we can see that
before Age was not significant at all, but now Age has two stars, meaning that
it's very significant in this new model.

This is due to something called *multicollinearity*.

### Correlation and Multicollinearity

Correlation measures the linear relationship between two variables and is 
a number between -1 and +1.

- `+1` = perfect positive linear relationship
- `0` = no linear relationship
- `-1` = perfect negative linear relationship

Look at some examples.

```{r}
plot(wine$WinterRain, wine$Price)
```

By visually inspecting the plot, it's hard to detect any linear relationship.
The correlation is 0.14, slight positive relationship.

```{r}
plot(wine$HarvestRain,wine$AGST)
```

It's again hard to visually see a linear relationship between the two variables,
and it turns out that the correlation is -0.06.

```{r}
plot(wine$Age, wine$FrancePop)
```

We can easily see that there's a strong negative linear relationship between these
two variables. This makes sense, since the population of France has increased
with time. The correlation is -0.99. These two variables are highly correlated.

To compute correlation in R using the cor() function.

```{r}
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
```

It's possible to compute the correlation between all pairs of variables in the
data. The output is a grid of numbers with the rows and columns labeled with the
varibles in the data set.

```{r}
cor(wine)
```

How does this information help us undestand the linear regression model?

It's confirmed that Age and FrancePop are definitely correlated.
There are also multicollinearity problems in the model that uses all of the 
available independent variables.

A high correlation between an independent variable and the dependent variable is
a good thing since we're trying to predict the dependent variable using the 
independent variable.

Due to the possibility of multicollinearyity, it's necessary to remove the 
insignificant variables one at a time. 

For example let's see what would have happened if we had removed both Age and 
FrancePop

```{r}
model5 = lm(Price ~ AGST + HarvestRain + WinterRain, data = wine)
summary(model5)
```

We can see that the R-squared is dropped to 0.75 respect the model that includes
Age that has an R-squared of 0.83. So if we had removed Age and FrancePop at the
same time, we would have missed a significant variable.

Why keep Age instead of FrancePop. Age respect a wine makes more intuitive sense
in the model.

Are there any other highly-correlated independent variables? There is no definitive
cut-off, but typicall a correlation greater than 0.7 is cause of concern.

### Making Predictions

An R-squared value of 0.83 for the model tell us that the model does a good job 
predicting the data.

But how well does the model perform on a new data?

The data that used to build a model is often called the *training data*, and the
new data is often called the *test data*.
The accuracy of the model on the test data is often called *out-of-sample accuracy*.

In the file wine_test.csv there are two data points that are not used to build the
model.

```{r}
wineTest = read.csv("wine_test.csv")
str(wineTest)
```

To make predictions we'll use the predict() function usingt the model4.

```{r}
predictTest = predict(model4, newdata = wineTest)
predictTest
```

It looks like the predictions are pretty good, but how quantify this by computing
the R-squared value.

