---
title: "Unit 3 - Recitation"
author: "GT"
date: "29 avril 2016"
output: html_document
---

## Unit 3 - Recitation

### Election Prediction

The topic of this recitation is electron forecasting, which is the art and science of predicting the winner
of an election before any votes are actually cast using polling data from likely voters.

In the United States, a president is elected every four years. Generally there are only two competitive
candidates. 

While in many countries the leader of the country is elected using the simple candidate who receives the largest
number of votes, in the US it's significantly more complicated. There are 50 states in the US and each is 
assigned a number of electoral votes based on its population. And these nubmer of electoral votes are
reassigned periodically based on changes of population between states. 

Within a given state in general, the system is winner take all in the sense that the candidate who receives 
the most voet in that state gets all of its electoral voets. And across the entire country, the candidate who
receives the most electoral votes wins the entire presidential election.

This can have very significant consequences on the outcome of the election.

In the 2000 presidential election, between G.Bush and Al Gore. Al Gore received more than 500000 more votes
across the entire country than G.Bush in terms of the popular vote. But in terms of the electoral vote, 
because of how those votes were distributed, G.Bush actually won the election because he received 5 more
electoral votes than Al Gore.

Our goal will be to use polling data that's collected from likely voters before the election to predict the 
winner in each state, and therefore to enable us to predict the winner of the entire election in the
electoral college system.

To carry out this prediction task, we're going to use some data from RealClearPolitics. com that basically
represents polling data that was collected in the months leading up to the 2004, 2008, nad 2012 US
presidential elections.

Instance represent a state in a given election

- *State*: Name of state
- *Year*: Election year (2004, 2008, 2012)

And the dependent variable

- *Republican*: 1 if Republican won state, 0 if Democrat won

Independent variables

- *Rasmussen, SurveyUSA*: Polled R% -Polled D%
- *DiffCount*: Polls with R winner - Polls with D winner
- *PropR*: Polls with R winner / # polls

Rasmussen and SurveyUSA variables are related to two major polls that are assigned across many different
states in the US. It represent the percentage of voters who said they were likely to vote Republicain minus
the percentage who said they were likely to vote Democrat.

### Dealing with Missing Data

```{r}
polling = read.csv("PollingData.csv")
str(polling)
```

Something we notice right off the bat is that even though there are 50 states and three election years,
we actually only have 145 obs in the data frame.

```{r}
table(polling$Year)
```

We see  that in 2012, only 45 of the 50 states have data. And what happened here is that pollsters were so
sure about the five missing states that they didn't perform any polls in the months leading up to the 2012.

So since these states are particularly easy to predict, we feel pretty confortable moving forward, making
predictions just for the 45 reamaining states.

The second thing that we notice is that there are these NA values, which signify missing data. 

```{r}
summary(polling)
```

We see that for the Rasmussen polling data and also for the SurveyUsa polling data, there are a decent
number of missing values.

There are a number of simple approaches to dealing with missing data. One would be to delete observations
that are missing at least on variable value. In this case that would result in throwing away more than 50%
of the observations.

Another way would be to remove the variable that have missing values. However, we expect Rasmussen and 
SurveyUsa to be qualitatively different from aggregate variables, such as DiffCount and PropR, so we want to
retain them in our data set.

A third approach would be to fill the misssing data points with average values. So for Rasmussen and 
SurveyUSA, the average value for a poll would be very close to zero across all the times with it reported,
which is roughly a tie between the Democrat and Republican canditate.

However, if PropR is very close to one or zero, we would expect the Rasmussen or SurveyUSA values that are
currently missing to be positive or negative, respectively.

This leads to a more complicated apporach called multiple imputation in which we fill in the missing
values based on the non-missing values for an obs.

For instance, if the Rasmussen variable is reported and very negative, then a missing SurveyUSU value would
likely be filled as a negative value as well.

Multiple imputation is in general a mathematically sophisticated approach, we can use it rather easily
through pre-existing R libraries. We will us Multiple Imputation by Chained Equation or mice package.

```{r}
library(mice)
```

So for our multiple imputation to be useful, we have to be able to find out the values of our missing 
variables without using the outcome of Republican. 

We're going to limit the data frame to just the 4 polling related variables before we actually perform
multiple imputation.

```{r}
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
```

If you ran the multiple imputation twice, you would get different values that were filled in. To be sure
that everybody gets the same results from imputation we're going to set the random seed to a value.

```{r}
set.seed(144)
imputed = complete(mice(simple))
```

The output here shows us that five rounds of imputation have been run, and nowe all of the variables
have been filled in.

```{r}
summary(imputed)
```

We can see that there's no more missing values.

The last step is to copy the Rasmussen and SurveyUSA variables back into the original polling data frame.

```{r}
imputed = read.csv("PollingData_Imputed.csv")
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
```

### A Sophisticated Baseline Method

Now we're ready to actually start building models. First thing to do is split our data into a training
and a testing set. For this problem, we're actually going to train on data from the 2004 and 2008 elections
and we're going to test on data from the 2012.

```{r}
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
```

We want to understand the prediction of our baseline model against which we want to compare a later logistic
regression model.

```{r}
table(Train$Republican)
```

We can see here that in 47 of the 100 training obs, the Democrat won the state, and in 53 of the obs, the
Republican won the state. So our simple baseline model is always going to predict the more common outcome,
which is that the Republican is going to win the state. And will have accuracy of 53% on the training set.

This is a pretty weak model. It always predicts Republican, even for a very landslide Demcratic state.
So nobody would really consider this to be a credible model.

We need to think a smarter baseline model against which we can compare our logistic regression model.

A reasonable smart baseline would be to just take one of the polls and make a prediction based on who
poll said was winning in the state. So to compute this smart baseline, we're going to use a new function
called the sign function.

```{r}
sign(20) # we pass a positive number --> 1
sign(-10) # we pass a negative number --> -1
sign(0) # we pass 0 --> 0
```

So if we passed the Rasmussen variable into sign, if the Republican was winning the state (Rasmussen positive),
it's going to return a `1`. If the Democrat is leading in the Rasmussen poll, it'll take on a negative value,
and we'll get `-1`. If the Rasmussen poll had a tie, it returns `0`, saying that the model is inconclusive
about who's going to win the state.

```{r}
table(sign(Train$Rasmussen))
```

What we can see is that in 56 of the 1000 training set obs, the smart baseline predicted that the
Republican was going to win. In 42 instances, it predicted the Democrat. And in two instances, it was
inconclusive.

So what we really want to do is to see the breakdown of how the smart baseline model does, compared to
the actual result --> who actually won the state.

```{r}
table(Train$Republican, sign(Train$Rasmussen))
```

In the table, the rows are the true outcome and the columns are the smart baseline predictions. What we
can see is in the top left corner we have 42 obs where the Rasmussen smart baseline predicted the Democrat
would win, and the Democrat actually did win. There were 52 obs. where the smart baseline predicted the 
Republican would win, and the Republicant actually win. There were those two inconclusive obs. And finally,
there were 4 mistakes. Four times where the smart baseline model predicted that the Republican would win,
but actually the Democrat won the state. 

This model is doing much, much better than the naive baseline, which simply was always predicting the
Republican would win and made 47 mistakes on the same data.

### Logistic Regression Model

