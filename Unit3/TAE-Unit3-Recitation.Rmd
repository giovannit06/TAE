---
title: "Unit 3 - Recitation"
author: "GT"
date: "29 avril 2016"
output: html_document
---

## Unit 3 - Recitation

### Election Prediction

The topic of this recitation is electron forecasting, which is the art and science of predicting the winner
of an election before any votes are actually cast using polling data from likely voters.

In the United States, a president is elected every four years. Generally there are only two competitive
candidates. 

While in many countries the leader of the country is elected using the simple candidate who receives the largest
number of votes, in the US it's significantly more complicated. There are 50 states in the US and each is 
assigned a number of electoral votes based on its population. And these nubmer of electoral votes are
reassigned periodically based on changes of population between states. 

Within a given state in general, the system is winner take all in the sense that the candidate who receives 
the most voet in that state gets all of its electoral voets. And across the entire country, the candidate who
receives the most electoral votes wins the entire presidential election.

This can have very significant consequences on the outcome of the election.

In the 2000 presidential election, between G.Bush and Al Gore. Al Gore received more than 500000 more votes
across the entire country than G.Bush in terms of the popular vote. But in terms of the electoral vote, 
because of how those votes were distributed, G.Bush actually won the election because he received 5 more
electoral votes than Al Gore.

Our goal will be to use polling data that's collected from likely voters before the election to predict the 
winner in each state, and therefore to enable us to predict the winner of the entire election in the
electoral college system.

To carry out this prediction task, we're going to use some data from RealClearPolitics. com that basically
represents polling data that was collected in the months leading up to the 2004, 2008, nad 2012 US
presidential elections.

Instance represent a state in a given election

- *State*: Name of state
- *Year*: Election year (2004, 2008, 2012)

And the dependent variable

- *Republican*: 1 if Republican won state, 0 if Democrat won

Independent variables

- *Rasmussen, SurveyUSA*: Polled R% -Polled D%
- *DiffCount*: Polls with R winner - Polls with D winner
- *PropR*: Polls with R winner / # polls

Rasmussen and SurveyUSA variables are related to two major polls that are assigned across many different
states in the US. It represent the percentage of voters who said they were likely to vote Republicain minus
the percentage who said they were likely to vote Democrat.

### Dealing with Missing Data

```{r}
polling = read.csv("PollingData.csv")
str(polling)
```

Something we notice right off the bat is that even though there are 50 states and three election years,
we actually only have 145 obs in the data frame.

```{r}
table(polling$Year)
```

We see  that in 2012, only 45 of the 50 states have data. And what happened here is that pollsters were so
sure about the five missing states that they didn't perform any polls in the months leading up to the 2012.

So since these states are particularly easy to predict, we feel pretty confortable moving forward, making
predictions just for the 45 reamaining states.

The second thing that we notice is that there are these NA values, which signify missing data. 

```{r}
summary(polling)
```

We see that for the Rasmussen polling data and also for the SurveyUsa polling data, there are a decent
number of missing values.

There are a number of simple approaches to dealing with missing data. One would be to delete observations
that are missing at least on variable value. In this case that would result in throwing away more than 50%
of the observations.

Another way would be to remove the variable that have missing values. However, we expect Rasmussen and 
SurveyUsa to be qualitatively different from aggregate variables, such as DiffCount and PropR, so we want to
retain them in our data set.

A third approach would be to fill the misssing data points with average values. So for Rasmussen and 
SurveyUSA, the average value for a poll would be very close to zero across all the times with it reported,
which is roughly a tie between the Democrat and Republican canditate.

However, if PropR is very close to one or zero, we would expect the Rasmussen or SurveyUSA values that are
currently missing to be positive or negative, respectively.

This leads to a more complicated apporach called multiple imputation in which we fill in the missing
values based on the non-missing values for an obs.

For instance, if the Rasmussen variable is reported and very negative, then a missing SurveyUSU value would
likely be filled as a negative value as well.

Multiple imputation is in general a mathematically sophisticated approach, we can use it rather easily
through pre-existing R libraries. We will us Multiple Imputation by Chained Equation or mice package.

```{r}
library(mice)
```

So for our multiple imputation to be useful, we have to be able to find out the values of our missing 
variables without using the outcome of Republican. 

We're going to limit the data frame to just the 4 polling related variables before we actually perform
multiple imputation.

```{r}
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
```

If you ran the multiple imputation twice, you would get different values that were filled in. To be sure
that everybody gets the same results from imputation we're going to set the random seed to a value.

```{r}
set.seed(144)
imputed = complete(mice(simple))
```

The output here shows us that five rounds of imputation have been run, and nowe all of the variables
have been filled in.

```{r}
summary(imputed)
```

We can see that there's no more missing values.

The last step is to copy the Rasmussen and SurveyUSA variables back into the original polling data frame.

```{r}
imputed = read.csv("PollingData_Imputed.csv")
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
```

### A Sophisticated Baseline Method

Now we're ready to actually start building models. First thing to do is split our data into a training
and a testing set. For this problem, we're actually going to train on data from the 2004 and 2008 elections
and we're going to test on data from the 2012.

```{r}
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
```

We want to understand the prediction of our baseline model against which we want to compare a later logistic
regression model.

```{r}
table(Train$Republican)
```

We can see here that in 47 of the 100 training obs, the Democrat won the state, and in 53 of the obs, the
Republican won the state. So our simple baseline model is always going to predict the more common outcome,
which is that the Republican is going to win the state. And will have accuracy of 53% on the training set.

This is a pretty weak model. It always predicts Republican, even for a very landslide Demcratic state.
So nobody would really consider this to be a credible model.

We need to think a smarter baseline model against which we can compare our logistic regression model.

A reasonable smart baseline would be to just take one of the polls and make a prediction based on who
poll said was winning in the state. So to compute this smart baseline, we're going to use a new function
called the sign function.

```{r}
sign(20) # we pass a positive number --> 1
sign(-10) # we pass a negative number --> -1
sign(0) # we pass 0 --> 0
```

So if we passed the Rasmussen variable into sign, if the Republican was winning the state (Rasmussen positive),
it's going to return a `1`. If the Democrat is leading in the Rasmussen poll, it'll take on a negative value,
and we'll get `-1`. If the Rasmussen poll had a tie, it returns `0`, saying that the model is inconclusive
about who's going to win the state.

```{r}
table(sign(Train$Rasmussen))
```

What we can see is that in 56 of the 1000 training set obs, the smart baseline predicted that the
Republican was going to win. In 42 instances, it predicted the Democrat. And in two instances, it was
inconclusive.

So what we really want to do is to see the breakdown of how the smart baseline model does, compared to
the actual result --> who actually won the state.

```{r}
table(Train$Republican, sign(Train$Rasmussen))
```

In the table, the rows are the true outcome and the columns are the smart baseline predictions. What we
can see is in the top left corner we have 42 obs where the Rasmussen smart baseline predicted the Democrat
would win, and the Democrat actually did win. There were 52 obs. where the smart baseline predicted the 
Republican would win, and the Republicant actually win. There were those two inconclusive obs. And finally,
there were 4 mistakes. Four times where the smart baseline model predicted that the Republican would win,
but actually the Democrat won the state. 

This model is doing much, much better than the naive baseline, which simply was always predicting the
Republican would win and made 47 mistakes on the same data.

### Logistic Regression Model

We need to consider first the possibility that there is multicollinearity within the independent
variables. And there's a good reason to suspect that there would be multicollinearity amongst the
variables, because in some sense, they're all measuring the same thing --> how strong the Republican
candidate is performing in the particular state.

If we do `cor(Train)` will get the error `'x' must be numeric`.

```{r}
str(Train)
```

When we look at the structure of the data, we see why we're getting this issue. It's because we're
trying to take the correlations of the names of states, which doesn't make any sense.

```{r}
cor(Train[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount", "Republican")])
```

SurveyUSA and Rasmussen are independent variables that have a correlation of 0.84, which is very large
and something that would be concerning. So combining them together isn't going to do much to produce a
working regression model.

Let's firt consider the case where we want to build a logistic regression model with just one variable.
In this case, it would be the variable that is most highly correlated with the outcome, Republican.

We see thet PropR is probabily the best candidate to include in our single-varible model, because it's
so highly correlated.

```{r}
mod1 = glm(Republican ~ PropR, data = Train, family = "binomial")
summary(mod1)
```

We can see that it looks pretty nice in terms of its significance and the sign of the coefficients.
We'll note down that the AIC measuring the strenght of the model is 19.8.

Let's see how it does in terms of actually predicting the Republican outcome on the training set.
First we want to compute the predictions, the predicted probabilities that the Republican is going
to win on the training set.

```{r}
pred1 = predict(mod1, type="response")
```

Now, we want to see how well it's doing.

```{r}
table(Train$Republican, pred1 >= 0.5)
```

We see that on the training set, this model with one variable as a prediction makes 4 mistakes, which
is just about the same as our smart baseline model. 

Let's see if we can improve on this performance by adding in another variable.

If we go back up to our correlations here, we're going to be searching for a pair of variables that
has a relatively lower correlation with each other, because they might kind of work together to 
improve the prediction overall of the Republican outcome.

The least correlated pairs of variables are either Rasmussen and DiffCount or SurveyUSA and DiffCount.
So the idea would be to try out one of these pairs in our two-variable model.

```{r}
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = "binomial")
summary(mod2)
pred2 = predict(mod2, type="response")
table(Train$Republican, pred2 >= 0.5)
```

We can see that we made one less mistake. We made 3 mistakes instead of 4 on the training. So a little
better than the smart baseline but nothing too impressive. If we look at the model (summary). We can
see that there are some things that are pluses.

The AIC has a smaller value, which suggests a stronger model. And SurveyUSA and DiffCount both have
positive coefficients in predicting if the Republican wins the state. But a weakness of this model
is that neither of these variables has a significance of a star or better, which means that they are
less significant statistically.

### Test Set Predictions

Now it's time to evaluate our models on the testing set. 

The first model we're going to want to look at is that smart baseline model that basically just took
a look at the polling results from the Rasmussen poll and used those to determine who was predicted to
win the election.

```{r}
table(Test$Republican, sign(Test$Rasmussen))
```

The results shows that 2 times are inconclusive and 4 mistakes.

This is going to be what we're going to compare our model against.

We need first to obtain final testing set prediction from the model.

```{r}
# we'll pass newdata = Test since we're actually making testing set predictions
TestPrediction = predict(mod2, newdata=Test, type="response") 
```
And, we're finally going to table the test set Republican value against the test prediction being
greater than or equal to 0.5.

```{r}
table(Test$Republican, TestPrediction >= 0.5)
```

We see that for this particular case, we make only one mistake. Now, we could have tried changing
this threshold from 0.5 to other values and computed out an ROC curve, bu that doesn't quite make as 
much sense in this setting where we're just trying to accurately predict the outcome of each staate
and we dont't care more about one sort of error.

Let's take a look now at the mistake we made and see if we can understand what's going on. So to 
actually pull out the mistake we made.

```{r}
subset(Test, TestPrediction >= 0.5 & Republican == 0)
```

This was the state of Florida for the year 2012. And looking through these predictor variables,
we see why we made the mistake. The Rasmussen poll gave the Republican a 2% point lead, SurveyUSA 
called a tie, DiffCount said that were 6 more polls that predicted Republican than Democrat, and two
third of the polls predicted the Republican was going to win. 

So the models here are not magic, and given this sort of data it's pretty unsurprising that our model
that our model actually didn't get Florida correct in this case and made the mistake.
