---
title: "Unit 3 - Lecture 1 - Modeling the Expert"
author: "GT"
date: "26 avril 2016"
output: html_document
---

## Unit 3 - Lecture 1 - Modeling the Expert

In this lecture, we'll examine how analytics can model an expert, in this case a physician, in the context
of assessing the quality of healthcare patients receive, and introduce a technique called *logistic regression*.

### Building the dataset

Medical claims are generated when a patient visits a doctor. Medical claims include diagnosis code, 
procedure code, as well as costs. Pharmacy claims involve drugs, the quantity of these drugs, the prescribing
doctor, as well as the medication costs.

For the dataset, we used a large health insurance claims database, and we randomly selected 131 diabetes 
patients. Age between 35 and 55 and the costs were near 10000-20000 $. The period for these claims were
recorded were September 1, 2003 to August 31, 2005.

An expert physician reviewed the claims and wrote descriptive notes, like "ongoing use of narcotics", "not a
good first choice drug". 

After this review, this expert rated the quality of care on a two-point scale, poor or good.

The dependent variable was the quality of care, the independent variables involve the descriptive notes of 
the physician expert. And also diabetes treatment variables, patient demographics, health care utilization,
providers, claims and prescriptions. 

The dependent variable was modeled as a binary variable --> 1 for low-quality care and 0 for high-quality care.

We will explain in this lecture how we can use *logistic regression*, which is an extension of linear
regression, to environments where the dependent variable is categorical.

### Logistic Regression

Logistic regression predicts the probability of the outcome variable being true. In this example,  we would
predict the probability that the patient is receiving poor care. 

We denote the PoorCare variable by y --> $P(y = 1)$.

And since the outcome is either 0 or 1, this means that the probabiility that the outcome variable is 0 is
$P(y = 0) = 1 - P(y = 1)$.

Just like in linear regression, we have a set of independent variables, $x_1, x_2, ..., x_k$ where $k$ is the
total number of independent variables.

To predict the probability $P(y = 1)$ we use what's called the **Logistic Response Function**.

$P(y = 1) = \frac{1}{1+e^-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k)}$

The Logistic Response Function is used to produce a number between 0 and 1. 

A positive coefficient value for a variable increases the linear regression piece, which increase the the
probability that y=1. On the other hand, a negative coefficient value for a variable decrease the linear 
regression piece, which in turn decrease the probability that y=1.

The coefficient are selected to predict a high probability for the actual poor care cases (y=1) and to
predict a low probability for the actual good care cases (y=0).

Another useful way to think about it is in terms of Odds, like in gambling.

$Odds=\frac{P(y=1)}{P(y=0)}$

- Odds > 1 if y = 1 is more likely
- Odds < 1 if y = 0 is more likely

If you substitute the Logistic Response Function for the probabilities in the Odds equation you can see that

$Odds=e^\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k$

By taking the log of both sides

$log(Odds)=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k$

The log(Odds) or Logit looks exactly like the linear regression equation.

- A positive beta value increases the Logit, which in turn increases the Odds of 1.
- A negative beta value decreases the Logit, which in turn decreases the Odds of 1.

### Logistic Regression in R

We'll build a logistic regression model in R to better predict poore car. Start reading the dataset in R.

```{r}
quality = read.csv("quality.csv")
str(quality)
```

We have 131 obs, one for each patients, and 14 different variables.



The variables in the dataset quality.csv are as follows:

- *MemberID*: numbers the patients from 1 to 131, and is just an identifying number.
- *InpatientDays*: is the number of inpatient visits, or number of days the person spent in the hospital.
- *ERVisits*: is the number of times the patient visited the emergency room.
- *OfficeVisits*: is the number of times the patient visited any doctor's office.
- *Narcotics*: is the number of prescriptions the patient had for narcotics.
- *DaysSinceLastERVisit*: is the number of days between the patient's last emergency room visit and the end
   of the study period (set to the length of the study period if they never visited the ER). 
- *Pain*: is the number of visits for which the patient complained about pain.
- *TotalVisits*: is the total number of times the patient visited any healthcare provider.
- *ProviderCount*: is the number of providers that served the patient.
- *MedicalClaims*: is the number of days on which the patient had a medical claim.
- *ClaimLines*: is the total number of medical claims.
- *StartedOnCombination*: is whether or not the patient was started on a combination of drugs to treat 
  their diabetes (TRUE or FALSE).
- *AcuteDrugGapSmall*: is the fraction of acute drugs that were refilled quickly after the prescription 
  ran out.
- *PoorCare*: is the outcome or dependent variable, and is equal to 1 if the patient had poor care, 
  and equal to 0 if the patient had good care.

Let's see how many patients received good care by using the table function

```{r}
table(quality$PoorCare)
```

We can see that 98 out of the 131 patients in the data set  received good care.

Before building any models, let's consider using a simple baseline method. 

When we computed the R-squared for linear regression, we compered our predictions to the baseline method
of predicting the average outcome for all data points.

In a classification problem, a standard baseline method is to just predict the most frequent outcome 
for all observations. Since good care is more common than poor care, in this case, we would predict that 
all patient are receiving good care. If we did this, we would get 98 out of the 131 obs correct, or have
an accuracy of about 75%. So the baseline has an accaracy of 75% and that is what we'll try to beat with
the logistic regression model.

In this lesson we have only a dataset, so we want to randomly split our dataset into a training set and
a testing set so that we'll have a test set to measure the out-of-sample accuracy.

We need to install a new package: *caTools* and load it in the R session.

```{r}
library(caTools)
```

Let's use now this package to randomly split the data inte a training set and testing set, using the
function sample.split. To make sure that we all get the same split, we'll set the seed. This initializes
the random number generator. 

```{r}
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75) # First argument, outcome variable
# Second argument, percentage of the data that we want in the training set.
```

The function makes sure that the outcome variable is well-balanced in each piece. So this function makes
sure that in our training set, 75% of our patients are receiving good care and in our testing set 75%
of our patients are receiving good care. In this way the test set is representative of the training set.

```{r}
str(split)
```

There is a TRUE or FALSE value for each of our obs. TRUE means that we should put that obs. in the
training set, and FALSE means that we should put that obs in the testing set. We do this using the 
subset function.

```{r}
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
str(qualityTrain)
str(qualityTest)
```

Now, we are ready to build a logistic regression model usig OfficeVisits and Narcotics as independent 
variables. We'll call the model QualityLog and use the glm() function for *generalized linear model* to
build the logistic regression model. The first argument is the equation (dependent var ~ independent var + 
... + independent var), the second argument is the name of the data. The last argument is `family=binomial`.
This tells the glm function to build a logist regression model.

```{r}
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
```

The output is similar to that of a linear regression model. We want focus on the coefficient table. Both
coefficient for OfficeVisits and Narcotics are both positive, which means that higher values in these
two variables are indicative of poor care. Furthermore, both of these variables have at least one star,
meaning that they're significant in the model.

The AIC value measure the quality of the model and is like Adjusted R-squared in that it accounts of the
number of variables used compared to the number of obs. Unfortunately, it can only be compared between
models on the same dataset. The preferred model is the on with the minimum AIC.

Let's make predictions using the model QualityLog

```{r}
predictTrain = predict(QualityLog, type="response") # Tells the predict to give us probabilities.
summary(predictTrain)
```

Since we're expecting probabilities, all of numbers should be between zero and one.

Let's see if we're predicting higher probabilities for the actual poor care cases as we expect. Use the 
tapply function, giving as arguments predictTrain and then qualityTrain$Poorcare and then mean.
This will compute the average prediction  for each of the true outcomes.

```{r}
tapply(predictTrain, qualityTrain$PoorCare, mean)
```

This is a good sign, beacause it look like we're predicting a higher probability for the actual poor
care cases.

### Thresholding

The outcome of a logistic regression model is a probability. Often, we want make an actual prediction.

We can convert the probabilities to predictions using what's called a *threshold value, t*.

If the probability of poor care is greater than this threshold value, t, we predict poor quality care.
If the probability of poor care is less han the threshold value, t, then we predict good quality care.

How we choose the threshold ? The threshold value is often selected based on which errors are better.

There are two type of errors that a model can make:

- one where you predict 1, or poor care, but the actual outcome is 0
- one where you predict 0, or good care, but the actual outcome is 1

With a *large* threshold value, we will predict poor care rarely, since the probability of poor care
has to be really large to be greater than the threshold, then we'll make more errors whene we say good care.

On the other hand, if the threshold value is *small*, we predict poor care frequently, and we predict
good care rarely. This means that we will make more errors where we say poor care, but it's actually good
care.

If there's no preference between the errors, the right threshold to select is `t = 0.5`, since it just
predicts the most likely outcome.

To make this discussion a little more quantitative, we use what's called a confusion matrix or classification
matrix. This compares the actual outcomes to the predict outcomes.

```
            Predicted = 0         Predicted = 1
Actual = 0  True Negatives (TN)   False Positives (FP)
Actual = 1  False Negatives (FN)  True Positives (TP)
```

The rows are labeled with the actual outcome, and the columns are labeled with the predicted outcome.
Each entry of the table gives the number of data observations that fall into that category.

We can compute two outcome measures that help us determine what types of errors we are making.

- $Sensitivity = \frac{TP}{TP + FN}$, measures the percentage of actual poor care that we classify correctly.
- $Specificity = \frac{TN}{TN + FP}$, measures the percentage of actual good care cases that we classify
  correctly.
  
A model with a higher threshold will have a lower sensitivity and a higher specificity.

A model with a lower threshold will have a higher sensitivity and a lower specificity.

Let's compute some confusion matrices in R using different threshold values. Make first some classification
tables using different threshold values and the table function. We'll use a threshold value of 0.5.

The firt argument is what we want to label the rows, the true outcome. And then the second argument is what
we want to label the columns, the predictions greater than 0.5.

```{r}
table(qualityTrain$PoorCare, predictTrain > 0.5)
```

For 70 cases, we predict good care and they actually received good care, and for 10 cases we predict poor
care and they actally received poor care. We made four mistakes where we say poor care and it's actually
good care, and we make 15 mistakes where we say good care, but it's actually poor care.

Let's compute the sensitivity and the specificity.

```{r}
10/25
70/74
```

Now, let's try to increase the threshold.

```{r}
table(qualityTrain$PoorCare, predictTrain > 0.7)
8/25
73/74
```

So by inrceasing the threshold, the sensitivity went down and the specificity went up.

Let's try decreasing the threshold.

```{r}
table(qualityTrain$PoorCare, predictTrain > 0.2)
16/25
54/74
```

So with the lower threshold, our sensitivity went up and our specificity went down. 
But which threshold should we pick ?

### ROC Curves

Picking a good threshold value is often challenging. A Receiver Operator Characteristic
curve or ROC curve, can help you decide which value is best.

The sensitivity, or true positive rate, is on the y-axis and 
the 1 minus specificity, or false positive rate, is on the x-axis.
The curve shows how these two outcome measures vary with different threshold values.

The ROC curve always starts at the point (0,0). This corresponds to threshold value
of 1 --> you will not catch any poor care cases (sensitivity = 0) and you have a false
positive rate of 0 (1 - specificity).

The ROC curve always ends at the point (1,1). This corresponds to a threshold value of
0 --> you'll catch all of the poor care cases (sensitivity = 0) and you have a false
positive rate of 1 (1 - specificity).

The threshold decreases as you move from (0,0) to (1,1).

At the point (0, 0.4) you're correctly labeling about 40% of the poor care cases with a
very small false positive rate.

At the point (0.6, 0.9) you're correctly labeling about 90% of the poor care cases, but
have a false positive rate of 60%.

At the point (0.3,0.8) you're correctly labeling about 80% of the poor care cases, with 
a 30% false positive rate.

The ROC curve captures all thresholds simultaneously. 

So which threshold value should you pick ?

You should select the best threshold fo the trade-off you want to make.

If you're concerned with having a high specificity or low false positive rate, pick
the threshold that maximizes the true positive rate while keeping the false positive
rate really low. A threshold around (0.1,0.5).

On the other hand, if your're more concerned with havin a high sensitivity or high
true positive rate, pick a threshold that minimizes the false positive rate but has a 
very high true positive rate. A threshold around (0.3, 0.8).

To generate ROC curves in R, we need to install the ROCR package. And load the package
using the library function.

We'll use the prediction function of ROCR. This function takes two arguments. 
The first is the predictions we made with our model (predictTrain).
The second argument is the true outcomes of our data points (qualityTrain$PoorCare)

```{r}
library(ROCR)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
```

Now, we need to use the performance function to define what we'd like to plot
on the x and y axis of the ROC curve.

```{r}
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf, colorize=TRUE)
```

Finally, let's add the threshold labels to the plot.

```{r}
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
```

### Interpreting the model

How to interpret the model developed.

One of the things we should look aftre is that might be what is called multicollinearity.

Multicollinearity occurs when the various independent variables are correlated, and this
might confuse the coefficients in the model.

So we have to check the correlations of independent variables and verify also that the
signs of the coefficients make sense. If it agrees with intuition, then multicollinearity
has not been a problem.

The next important thing is significance.

So how do we interpret the results, and how we understand wheter we have a good model
or not?

Let's look at what is called Area Under the Curve of ROC, or AUC. The AUC shows an
absolute measure of quality of prediction. The perfect score is 100% whereas a 50% score
is pure guessing. In this case 77.5% is a 50% of success.

Other outcome measures that are important to discuss is the so called confusion matrix.
The actual class = 0 means, good quality of care and the actual class = 1 means poor
quality of care. The predicted class = 0 means that will predict good quality and the
predicted class = 1 means that we predict poor quality.

```
            Predicted = 0         Predicted = 1
Actual = 0  True Negatives (TN)   False Positives (FP)
Actual = 1  False Negatives (FN)  True Positives (TP)

N = number of observations
```

$Overall Accuracy = \frac{TN + TP}{N}$

$Overall Error Rate = \frac{FP + FN}{N}$

$Sensitivity = \frac{TP}{TP + FN}$

This is total number of times that we predict poor quality, and it is, indeed, poor
quality, versus the total number of times the actual quality is, in fact, poor.

$False Negative Error Rate = \frac{FN}{TP + FN}$

$Specificity = \frac{TN}{TN + FP}$

This is the number of times we predict good quality and, in fact, quality is good,
versus the toal number of times the actual quality is good.

$False Positive Error Rate = \frac{FP}{TN + FP}$

In this particular example we want to make predictions on a test set to compute 
out-of-sample metrics.

```{r}
predictTest = predict(QualityLog, type="response", newdata = qualityTest)
table(qualityTest$PoorCare, predictTest > 0.3)
```

In this way, we make predictions about probabilities, of course, simply because logistic
regression makes predictions about probabilities and then we transform them to a binary
outcome --> the quality is good or the quality is poor, using a threshold.

The overall accuracy of the model is `(19+6)/32`.

The false positive rate is `5/24`.

The true positive rate is `6/8`.

Comparing this model with one alternative where we predict good care all the time.
In that situation we will be correct 24/25 times. But notice that predicting always good
care does not capture the dynamics of what is happening, versus the logistic regression
model that is far more intelligent in capturing these effects.

```{r}
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
```

### The Analytic Edge

We built an expert-trained model by a physician that can accurately identify diabetic
patients receiving low quality care.

The out-of-sample accuracy was 78%, but most importantly, the model identifies moste
patients receiving poor care, which is the major objective in the study.

The model provide probabilities of somebody receiving poor quality care. These 
probabilities can be used to prioritize patients for intervention.
