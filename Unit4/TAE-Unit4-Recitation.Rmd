---
title: "TAE - Unit 4 - Recitation"
author: "GT"
date: "10 mai 2016"
output: html_document
---

## Location, Location, Location

In this recitation, we will be looking at regression trees, and applying them to data related
to house prices and locations.

### Boston Housing Data

In real estate, there is a famous saying that the most important thing is location, location, location.

Boston is the capital of the state of Massachusetts, USA. It was first settled in 1630, and in the
greater Boston area there are about 5 million people. The area features some of the highest population
densities in America.

In this recitation, we will be talking about Boston in a sense of the greater Boston area.
Over the greater Boston area, the nature of the housing varies widely.

This data comes from a paper, "Hedonic Housing Prices and the Demand for Clean Air," which
has been cited more than 1,000 times. This paper was written on a relationship between house
prices and clean air in the late 1970s by David Harrison of Harvard and Daniel Rubinfeld
of the University of Michigan.

The data set is widely used to evaluate algorithms of a nature we discuss in this class.

In the lecture, we mostly discuss *classification trees* with the output as a factor or a category.

Trees can also be used for regression tasks. The output at each leaf of a tree is no longer a category, 
but a number.

Just like classification trees, regression trees can capture nonlinearities that linear regression can't.

So what does that mean?

Well, with classification trees we report the average outcome at each leaf of our tree.

For example, if the outcome is true 15 times, and false 5 times, the value at that leaf of a tree would be
15/(15+5)=0.75. Now, if we use the default threshold of 0.5, we would say the value at this leaf is true.

With regression trees, we now have continuous variables. We report the average of the values at that leaf.

So suppose we had the values 3, 4, and 5 at one of the leaves of our trees. Well, we just take the average
of these numbers, which is 4, and that is what we report.

That might be a bit confusing so let's look at a picture.

```
  ^
  |   xxx
8 |  xxxx
  |   xx x                  x
6 |                          x xx x
  |                        x xxx
4 |           xxx
  |           xx x
2 |          x x x
  |
  |--------------------------------->
        10         20         30
```

So if we fit a linear regression to this data set, it does not do very well on this data set.

However, we can notice that the data lies in three different groups.
If we draw these lines here, we see x is either less than 10, between 10 and 20, or greater then 20,
and there is very different behavior in each group.

Regression trees can fit that kind of thing exactly. So the splits would be x is less than or equal to 10,
take the average of those values. x is between 10 and 20, take the average of those values. x is between 20 
and 30, take the average of those values.

We see that regression trees can fit some kinds of data very well that linear regression completely fails on.

In this recitation, we will explore the data set with the aid of trees. We will compare linear regression
with regression trees. We will discuss what the cp parameter means that we brought up when we did 
cross-validation in the lecture, and we will apply cross-validation to regression trees.

### The Data

- Each entry of this data set corresponds to a census **tract**, a statistical division of the area that
is used by researchers to break down towns and cities. 
- There will usually be multiple census tracts per **town**.
- **LON** and **LAT** are the longitude and latitude of the center of the census tract.
- **MEDV** is the median value of owner-occupied homes, measured in thousands of dollars.
- **CRIM** is the per capita crime rate. 
- **ZN** is related to how much of the land is zoned for large residential properties.
- **INDUS** is the proportion of the area used for industry.
- **CHAS** is 1 if a census tract is next to the Charles River.
- **NOX** is the concentration of nitrous oxides in the air, a measure of air pollution.
- **RM** is the average number of rooms per dwelling.
- **AGE** is the proportion of owner-occupied units built before 1940.
- **DIS** is a measure of how far the tract is from centers of employment in Boston.
- **RAD** is a measure of closeness to important highways.
- **TAX** is the property tax per $10,000 of value.
- **PTRATIO** is the pupil to teacher ratio by town.

So let's begin to analyze our data set with R. First of all, we'll load the data set into the Boston variable.
If we look at the structure of the Boston data set,we can see all the variables we talked about before.

```{r}
boston = read.csv("boston.csv")
str(boston)
```

There are 506 observations corresponding to 506 census tracts in the Greater Boston area.

We are interested in building a model initiallyof how prices vary by location across a region.

So let's first see how the points are laid out.Using the plot commands, we can plot the latitude and longitude
of each of our census tracts.

```{r}
plot(boston$LON, boston$LAT)
```

This picture might be a little bit meaningless to you if you're not familiar with the Massachusetts-Boston area,
but I can tell you that the dense central core of points corresponds to Boston city, Cambridge city, and 
other close urban cities.

So we want to show all the points that lie along the Charles River in a different color.
We have a variable, CHAS, that tells us if a point is on the Charles River or not.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS==1],boston$LAT[boston$CHAS==1], col="blue", pch=19)
```

So by running this command, you see we have some blue dots in our plot now. These are the census
tracts that lie along the Charles River. But maybe it's still a little bit confusing, and you'd like
to know where MIT is in this picture.

The census tract MIT is tract 3531. So let's plot that.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS==1],boston$LAT[boston$CHAS==1], col="blue", pch=19)
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531], col="red", pch=19)
```

Can you see it on the little picture? It's a little red dot, right in the middle.

What other things can we do?

Well, this data set was originally constructed to investigate questions about how air pollution 
affects prices. So the air pollution variable is this NOX variable. Let's have a look at a distribution
of NOX.

```{r}
summary(boston$NOX)
```

So we see that the minimum value is 0.385, the maximum value is 0.87 and t
he median and the mean are about 0.53, 0.55.
So let's just use the value of 0.55, as it's kind of in the middle. And we'll look at just the census
tracts that have above-average pollution.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS==1],boston$LAT[boston$CHAS==1], col="blue", pch=19)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=19)
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531], col="red", pch=19)
```


So those are all the points that have got above-average pollution. Looks like my office is right
in the middle. Now it kind of makes sense, though, because that's the dense urban core of Boston.
If you think of anywhere where pollution would be, you'd think it'd be where the most cars and the 
most people are.

Now, before we do anything more, we should probably look at how prices vary over the area as well.
If we look at the distribution of the housing prices (boston$MEDV),

```{r}
summary(boston$MEDV)
```

we see that the minimum price, units are thousands of dollars, is around five,
the maximum is around 50.

So let's plot again only the above-average price points.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2],col="red", pch=19)
```

So what we see now are all the census tracts with above-average housing prices.
As you can see, it's definitely not simple.

The census tracts of above-average and below-average are mixed in between each other.
So there's definitely some structure to it, but it's certainly not simple in relation
to latitude and longitude at least.

### Geographical Predictions

We saw that the house prices were distributed over the area in an interesting way,
certainly not the kind of linear way. And we wouldn't necessarily expect linear regression
to do very well at predicting house price, just given latitude and longitude.

We can kind of develop that intuition more by plotting the relationship between 
latitude and house prices or the longitude and the house prices. They look pretty nonlinear.

```{r}
plot(boston$LAT, boston$MEDV)
plot(boston$LON, boston$MEDV)
```

So, we'll try fitting a linear regression anyway. And we'll use the lm command, linear model,
to predict house prices based on latitude and longitude using the boston data set.

```{r}
latlonlm = lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)
```

If we take a look at our linear regression, we see the R-squared is around 0.1, which is not 
great. The latitude is not significant, which means the north-south differences aren't
going to be really used at all. Longitude is significant, and it's negative.
Which we can interpret as, as we go towards the ocean, towards the east, house prices decrease 
linearly.

So this all seems kind of unlikely, but let's work with it.

So let's see how this linear regression model looks on a plot.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2],col="red", pch=19)
```

Now, the question we're going to ask, and then plot, is what does the linear regression model
think is above median. So we could just do this pretty easily.

We have latlonlm$fitted.values and this is what the linear regression model predicts for each 
of the 506 census tracts.

```{r}
head(latlonlm$fitted.values,20)
```

So we'll plot these on top. If we use the dots again, we'll cover up the red dots
and cover up some of the black dots. What we won't be able to see is where
the red dots and the blue dots match up. It turns out that you can actually
pass in characters to this PCH option.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2],col="red", pch=19)
points(boston$LON[latlonlm$fitted.values>=21.2],boston$LAT[latlonlm$fitted.values>=21.2],col="blue", pch="$")
```

So, the linear regression model has plotted a dollar sign for every time it thinks the census
tract is above median value. And you can see that, indeed, it's almost as you can see the sharp line
that the linear regression defines.

And how it's pretty much vertical, because remember before, the latitude variable
was not very significant in the regression. So that's interesting and pretty wrong.

One thing that really stands out is how it says Boston is mostly above median. Even  
we saw it right from the start, there's a big non-red spot, right in the middle of Boston, 
where the house prices were below the median.

So the linear regression model isn't really doing a good job. And it's completely ignored 
everything to the right side of the picture.

### Regression Trees

Let's see how regression trees do. We'll first load the rpart library and also load the rpart 
plotting library.

```{r}
library(rpart)
library(rpart.plot)
```

We build a regression tree in the same way we would build a classification tree, using the 
rpart command. We predict MEDV as a function of latitude and longitude, using the boston dataset.

```{r}
latlontree = rpart(MEDV ~ LAT + LON, data=boston)
prp(latlontree)
```

In the plot the tree using the prp command, which is defined in rpart.plot, we can see it 
makes a lot of splits and is a little bit hard to interpret.
But the important thing is to look at the leaves.

In a classification tree, the leaves would be the classification we assign that these splits
would apply to.

But in regression trees, we instead predict the number. That number is the average of the median house
prices in that bucket or leaf.

We'll plot again the latitude of the points. And we'll again plot the points with above median 
prices. Then we want to predict what the tree thinks is above median, just like we did with 
linear regression. So we'll say the fitted values we can get from using the predict command 
on the tree we just built.

```{r}
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2],col="red", pch=19)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>=21.2],boston$LAT[fittedvalues>=21.2],col="blue", pch="$")
```

Now we see that we've done a much better job than linear regression was able to do.
We've correctly left the low value area in Boston and below out, and we've correctly
managed to classify some of those points in the bottom right and top right.

We're still making mistakes, but we're able to make a nonlinear prediction on latitude and longitude.

So that's interesting, but the tree was very complicated. Maybe it's drastically overfitting.

Can we get most of this effect with a much simpler tree? We can.

We would just change the minbucket size. So let's build a new tree.

```{r}
latlontree = rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)
```

And we see we have far fewer splits, and it's far more interpretable.

The first split says if the longitude is greater than or equal to negative 71.07,
so if you're on the right side of the picture.

So the left-hand side of the tree corresponds to the right-hand side of the map.
And the right side of the tree corresponds to the left side of the map.

Let's see what it means visually. So we'll remember these values, and we'll
plot the longitude and latitude again.

```{r}
plot(boston$LON, boston$LAT)
# first split was on longitude = -71.07
abline(v=-71.07)
```

It corresponds to being on either the left or right-hand side of this tree.

We'll focus on the lowest price prediction, which is in the bottom left corner of the tree,
right down at the bottom left after all those splits. So that's where we want to get to.

So let's plot again the points.

```{r}
plot(boston$LON, boston$LAT)
# first split was on longitude = -71.07
abline(v=-71.07)
# next split on latitude = 42.21, Charles River
abline(h=42.21)
# final split on latitude = 42.17
abline(h=42.17)
```

And now that's interesting. If we look at the right side of the middle of the three
rectangles on the right side, that is the bucket we were predicting. And it corresponds to 
the South Boston low price area we saw before. So maybe we can make that more clear by plotting, now,
the high value prices.

```{r}
plot(boston$LON, boston$LAT)
abline(v=-71.07)
abline(h=42.21)
abline(h=42.17)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2],col="red", pch=19)
```

So this makes it even more clear.

We've correctly shown how the regression tree carves out that rectangle in the bottom of Boston
and says that is a low value area. So that's actually very interesting. It's shown us something 
that regression trees can do that we would never expect linear regression to be able to do.

So the question we're going to answer is given that regression trees can do these fancy things
with latitude and longitude, is it actually going to help us to be able to build a predictive model,
predicting house prices? Well, we'll have to see.

### Putting it all Together

But what really matters at the end of the day is whether it can predict things better than linear 
regression.

We're going to try to predict house prices using all the variables we have available to us.

```{r}
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio=0.7)
train = subset(boston, split==TRUE)
test = subset(boston, split==FALSE)
```


First of all, let's make a linear regression model, nice and easy.

```{r}
linreg = lm(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data = train)
summary(linreg)
```

So we see that the latitude and longitude are not significant for the linear regression, which is
perhaps not surprising because linear regression didn't seem to be able to take advantage of them.
Crime is very important. The residential zoning might be important. Whether it's on the Charles River
or not is a useful factor. Air pollution does seem to matter, the coefficient is negative, as you'd expect.
The average number of rooms is significant. The age is somewhat important. Distance to centers of 
employment (DIS), is very important. Distance to highways and tax is somewhat important, and the pupil-teacher 
ratio is also very significant.

Some of these might be correlated, so we can't put too much stock in necessarily interpreting
them directly, but it's interesting.

The adjusted R squared is 0.65, which is pretty good.

So because it's kind of hard to compare out of sample accuracy for regression, we need to think 
of how we're going to do that.

With classification, we just say, this method got X% correct and this method got Y% correct.

Well, since we're doing continuous variables, let's calculate the sum of squared error, which
we discussed in the original linear regression video.

```{r}
linreg.pred = predict(linreg, newdata=test)
linreg.sse = sum((linreg.pred - test$MEDV)^2)
linreg.sse
```

The linear regression's predictions are predict(linreg, newdata=test) and the linear regression
sum of squared errors is simply the sum of the predicted values versus the actual values squared.

We're interested to see now is, can we beat this using regression trees?

So let's build a tree.

```{r}
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data = train)
prp(tree)
```

So again, latitude and longitude aren't really important as far as the tree's concerned.
The rooms are the most important split. Pollution appears in there twice, so it's, in some sense,
nonlinear on the amount of pollution, if it's greater than a certain amount
or less than a certain amount, it does different things. Crime is in there, age is in there.
Room appears three times. That's interesting. So it's very nonlinear on the number of rooms.

Things that were important for the linear regression that don't appear in ours include pupil-teacher 
ratio. The DIS variable doesn't appear in our regression tree at all, either.

So they're definitely doing different things, but how do they compare?

So we'll predict, again, from the tree. And the tree sum of squared errors is the sum of the 
tree's predictions versus what they really should be.

```{r}
tree.pred = predict(tree, newdata=test)
tree.sse = sum((tree.pred - test$MEDV)^2)
tree.sse
```


So, simply put, regression trees are not as good as linear regression for this problem.
What this says to us, given what we saw with the latitude and longitude, is that latitude and 
longitude are nowhere near as useful for predicting, apparently, as these other variables are.

That's just the way it goes, I guess. It's always nice when a new method does better,

### The CP Parameter

**cp** stands for complexity parameter.

Recall that the first tree we made using latitude and longitude only had many splits,
but we were able to trim it without losing much accuracy.

The intuition we gain is, having too many splits is bad for generalization so we should 
penalize the complexity.

Let us define RSS to be the residual sum of squares, also known as the sum of square differences.

$$RSS=\sum_{i=1}^{n}(y_i-f(x_x))^2$$

Our goal when building the tree is to minimize the RSS by making splits, but we want to 
penalize having too many splits now. Define $S$ to be the number of splits, and 
$\lambda$ (lambda) to be our penalty. Our new goal is to find a tree that minimizes 

$$\sum_{Leaves}(RSS_{at Each Leaf})+\lambda S$$

Let us consider the following example.

Here we have set lambda to be equal to 0.5.

```
.-----------------------------------------------.
|           |              |                    |
|   Splits  |      RSS     |    Total Penalty   |
|           |              |                    |
-------------------------------------------------
|           |              |                    |
|     0     |       5      |          5         |
|           |              |                    |
-------------------------------------------------
|           |              |                    |
|     1     |   2 + 2 = 4  |   4 + 0.5*1 = 4.5  |
|           |              |                    |
-------------------------------------------------
|           |              |                    |
|     2     |  1+0.8+2=3.8 |  3.8 + 0.5*2 = 4.8 |
|           |              |                    |
'-----------|--------------|--------------------'
                                      
```

Initially, we have a tree with no splits. We simply take the average of the data.
The RSS in this case is 5, thus our total penalty is also 5.

If we make one split, we now have two leaves. At each of these leaves, say, we have an error, 
or RSS of 2. The total RSS error is then 2+2=4. And the total penalty is 4+0.5*1, the number 
of splits. Our total penalty in this case is 4.5.

If we split again on one of our leaves, we now have a total of three leaves for two splits.
The error at our left-most leaf is 1. The next leaf has an error of 0.8. And the next leaf 
has an error of 2, for a total error of 3.8. The total penalty is thus 3.8+0.5*2,
for a total penalty of 4.8.

$$\sum_{Leaves}(RSS_{at Each Leaf})+\lambda S$$

Notice that if we pick a large value of lambda, we won't make many splits, because you
pay a big price for every additional split that will outweigh the decrease in error.

If we pick a small, or 0 value of lambda, it will make splits until it no longer 
decreases the error. 

You may be wondering at this point, the definition of cp is what, exactly?

Well, it's very closely related to lambda.

Considering a tree with no splits, we simply take the average of the data,
calculate RSS for that so-called tree, and let us call that RSS for no splits.

$$c_p=\frac{\lambda}{RSS(noSplits)}$$

When you're actually using cp in your R code, you don't need to think exactly what it means
--> just that small numbers of cp encourage large trees, and large values of cp encourage 
small trees.

### Cross-Validation

OK, so now we know what CP is, we can go ahead and build one last tree using cross validation.

So we need to make sure first we have the required libraries "caret" and "e1071".
and we need to tell the caret package how exactly we want to do our parameter tuning.
But we're going to restrict ourselves in this course to just 10-fold cross validation,
as was explained in the lecture.

```{r}
library(caret)
library(e1071)
tr.control=trainControl(method="cv", number=10)
```


Now we need to tell caret which range of cp parameters to try out.

Now remember that cp varies between 0 and 1. It's likely for any given problem
that we don't need to explore the whole range. The value of cp we're going to pick is very small.

```{r}
cp.grid = expand.grid(.cp = (0:10)*0.001)
```

So those are the values of cp that caret will try.

So let's store the results of the cross validation fitting in a variable called tr.

```{r}
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train, method="rpart", trControl=tr.control, tuneGrid = cp.grid)
tr
```


You can see it tried 11 different values of cp.
And it decided that cp equals 0.001 was the best because it had the best RMSE-- Root Mean 
Square Error.

It's interesting though that the numbers are so low. I tried it for a much larger range 
of cp values, and the best solutions are always very close to 0. So it wants us to build a 
very detail-rich tree.

So let's see what the tree that that value of cp corresponds to is.

```{r}
best.tree = tr$finalModel
prp(best.tree)
```

Wow, OK, so that's a very detailed tree. You can see that it looks pretty much like 
the same tree we had before, initially. But then it starts to get much more detailed 
at the bottom. And in fact if you can see close enough, there's actually latitude and
longitude in there right down at the bottom as well.

So maybe the tree is finally going to beat the linear regression model. Well, we can 
test it out the same way as we did before.

```{r}
best.tree.pred = predict(best.tree, newdata=test)
best.tree.sse = sum((best.tree.pred - test$MEDV)^2)
best.tree.sse
```

That number is 3,675. So if you can remember the tree from the previous model
actually only got something in the 4,000s.

So we have actually improved. This tree is better on the testing set than the original
tree we created.

But, you may also remember that the linear regression model did actually better than that still.
The linear regression SSE was more around 3,030.

So the best tree is not as good as the linear regression model. But cross validation 
did improve performance.

So the takeaway is, I guess, that trees aren't always the best method you have available 
to you. But you should always try cross validating them to get as much performance out of
them as you can.
