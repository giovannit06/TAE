---
title: "TAE - Unit 6 - Lecture 1"
author: "GT"
date: "18 mai 2016"
output: html_document
---

## Reccomendations Worth a Million

In this lecture, we'll be discussing the story of Netflix and how their recommendation 
system is worth a million dollars. 

Through this example, we'll introduce the method of *clustering*.

Netflix is an online DVD rental and streaming video service. Customers can receive movie 
rentals by mail, and they can also watch selected movies and TV shows online.

Netflix has more than 40 million subscribers worldwide and has an annual revenue of
$3.6 billion.

A key aspect of the company is being able to offer customers accurate movie recommendations 
based on a customer's own preferences and viewing history.

From 2006 through 2009, Netflix ran a contest asking the public to submit algorithms to 
predict user ratings for movies. This algorithm would be useful for Netflix when making 
recommendations to users.

Netflix provided a training data set of about 100 million user ratings and a test data set
of about three million user ratings.

They offered a grand prize of one million dollars to the team who could beat Netflix's 
current algorithm, called Cinematch, by more than 10% measured in terms of root mean squared 
error.

Netflix believed that their recommendation system was so valuable that it was worth a 
million dollars to improve.

The contest had a few rules. 

One was that if the grand prize was not yet reached, progress prizes of $50,000 per year
would be awarded for the best result so far, as long as it was at least a 1% improvement
over the previous year.

Another rule was that teams must submit their code and a description of the algorithm
to be awarded any prizes.

And lastly, if a team met the 10% improvement goal, a last call would be issued, and 30 days
would remain for all teams to submit their best algorithm.

So what happened?

The contest went live on October 2, 2006. 

- By October 8, only six days later, a team submitted an algorithm that beat Cinematch. 
- A week later, on October 15, there were three teams already submitting algorithms beating 
  Cinematch.
- One of these solutions beat Cinematch by more than 1%, already qualifying for a progress prize.

The contest was hugely popular all over the world. By June 2007, over 20,000 teams had
registered from over 150 countries. The 2007 progress prize went to a team called BellKor,
with an 8.43% improvement over Cinematch. The following year, several teams from across 
the world joined forces to improve the accuracy even further.

In 2008, the progress prize again went to team BellKor. But this time, the team included 
members from the team BigChaos in addition to the original members of BellKor.

This was the last progress prize because another 1% improvement would reach the grand prize 
goal of 10%. On June 26, 2009, the team BellKor's Pragmatic Chaos, composed of members
from three different original teams, submitted a 10.05% improvement over Cinematch,
signaling the last call for the contest.

Other teams had 30 days to submit algorithms before the contest closed. These 30 days were 
filled with intense competition and even more progress.

### Reccomendation Systems

Now, we'll discuss how recommendation systems work.

Let's start by thinking about the data. When predicting user ratings, what data could 
be useful?

There are two main types of data that we could use.

The first is that for every movie in Netflix's database, we have a ranking from all users 
who have ranked that movie.

The second is that we know facts about the movie itself, the actors in the movie, 
the director, the genre classifications of the movie, the year it was released, et cetera.

As an example, suppose we have the following user ratings for four users and four movies.
As we discussed in the previous video,
Our users are Amy, Bob, Carl, and Dan. And our movies are Men in Black, Apollo 13, Top Gun,
and Terminator. The ratings are on a one to five scale, where one is the lowest
rating and five is the highest rating. The blank entries mean that the user has not 
rated the movie.


```
------------------------------------------------------
     | Men in Black | Apollo 13 | Top Gun | Terminator
------------------------------------------------------
Amy  |       5      |      4    |     5   |    4
------------------------------------------------------
Bob  |       3      |           |     2   |    5
------------------------------------------------------
Carl |              |      5    |     4   |    4
------------------------------------------------------
Dan  |       4      |      2    |         |  
------------------------------------------------------
```


We could suggest to Carl that he watch Men in Black, since Amy rated it highly. 
She gave it a rating of five, and Amy and Carl seem to have similar ratings
for the other movies. 

This technique of using other user's ratings to make predictions is called 
**collaborative filtering**.

Note that we're not using any information about the movie itself here, just the
similarity between users.

Instead, we could use movie information to predict user 

We saw in the table that Amy liked Men in Black. She gave it a rating of five.
We know that this movie was directed by Barry Sonnenfeld, is classified in the 
genres of action, adventure, sci-fi, and comedy, and it stars actor Will Smith.
Based on this information, we could make recommendations to Amy.

We could recommend to Amy another movie by the same director, Barry Sonnenfeld's
movie, Get Shorty. We can instead recommend the movie Jurassic Park, which is also 
classified in the genres of action, adventure, and sci-fi. Or we could recommend to 
Amy another movie starring Will Smith, Hitch.

Note that we're not using the ratings of other users at all here, just information 
about the movie.

This technique is called **content filtering**.

There are strengths and weaknesses to both types of recommendation systems.

*Collaborative filtering* 

- can accurately suggest complex items without understanding the nature of the items.
  We were just comparing user ratings.
- this requires a lot of data about the user to make accurate recommendations.
- when there are millions of items, it needs a lot of computing power to compute the
  user similarities.
  
*Content filtering*

- requires very little data to get started.
- But the major weakness of content filtering is that it can be limited in scope.
  You're only recommending similar things to what the user has already liked.
  So the recommendations are often not surprising or particularly insightful.

Netflix actually uses what's called a **hybrid recommendation system**.

They use both collaborative and content filtering. 

As an example, consider a collaborative filtering approach, where we determine that Amy 
and Carl have similar preferences. We could then do content filtering as well,
where we could find that the movie Terminator, which they both liked, is classified in 
almost the same set of genres as Starship Troopers. 

So then we could recommend Starship Troopers to both Amy and Carl, even though neither
of them have seen it before.

If we were only doing collaborative filtering, one of them would have had to have seen it 
before. And if we were only doing content filtering, we would only be recommending to one 
user at a time.

So by combining the two methods, the algorithm can be much more efficient and accurate.

### Movie Data and Clustering

In this lecture, we'll be using data from MovieLens to explain clustering and perform 
content filtering.

movielens.org is a movie recommendation website run by the GroupLens research lab
at the University of Minnesota.

They collect user preferences about movies and do collaborative filtering to make 
recommendations to users, based on the similarities between users.

We'll use their movie database to do content filtering using a technique called **clustering**.

Movies in the MovieLens data set are categorized as belonging to different genres.
There are 18 different genres as well as an unknown category.

The genres include crime, musical, mystery,and children's.

Each movie may belong to many different genres. So a movie could be classified as drama, adventure,
and sci-fi.

The question we want to answer is, can we systematically find groups of movies with similar 
sets of genres?

To answer this question, we'll use a method called *clustering*.

Clustering is different from the other analytics methods we've covered so far.

It's called an **unsupervised** learning method.

This means that we're just trying to segment the data into similar groups,
instead of trying to predict an outcome.

```
^
|
|     *
|   ****
|  **** *          oooo
| *  *           ooo o
|             oooo
|
|
|    x
|     xxxx
|   x x x
|     x
+----------------------------->
```

In this graph, based on the locations of points, we've divided them into three clusters, 
a '*' cluster, a 'o' cluster, and a 'x' cluster.

This is the goal of clustering, to put each data point into a group with similar values 
in the data. A clustering algorithm does not predict anything. 

However, clustering can be used to improve predictive methods. You can cluster the data into
similar groups and then build a predictive model for each group. This can often improve the 
accuracy of predictive methods. But as a warning, be careful not to over-fit your model
to the training set.

This works best for large data sets.

There are many different algorithms for clustering.

They differ in what makes a cluster and how the clusters are found.

In this class, we'll cover 

- hierarchical clustering
- K-means clustering.

There are other clustering methods also, but hierarchical and K-means are two
of the most popular methods. To cluster data points, we need to compute
how similar the points are.

### Computing Distnces

So how does clustering work?

The first step in clustering is to define the distance between two data points.

The most popular way to compute the distance is what's called Euclidean distance.
This is the standard way to compute distance that you might have seen before.

Suppose we have two data points, $i$ and $j$. The distance between the two points,
which we'll call $d_{ij}$, is equal to

$$d_{ij}=\sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+...+(x_{ik}-x_{jk})^2}$$

where $k$ here is the number of attributes or independent variables.

Let's see how this works by looking at an example.

In our movie lens dataset, we have binary vectors for each movie, classifying that movie 
into genres.

The movie 'Toy Story' is categorized as an animation, comedy, and children's movie.
So the data for Toy Story has (0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0).

The movie 'Batman Forever' is categorized as an action, adventure, comedy, and crime movie.
So Batman Forever has (0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0).

So given these two data observations, let's compute the distance between them.

$$d=\sqrt{(0-0)^2+(0-1)^2+(0-1)^2+(1-0)^2+...}=\sqrt{5}$$

In addition to Euclidean distance, there are many other popular distance
metrics that could be used.

One is called *Manhattan distance*, where the distance is computed to be the sum of 
the absolute values instead of the sum of squares.

Another is called *Maximum Coordinate distance*, where we only consider the measurement 
for which the data points deviate the most.

Another important distance that we have to calculate for clustering is the 
*Distance Between Clusters*, when a cluster is a group of data points.

How do we compute the distance between groups of points?

One way of doing this is by using what's called the *minimum distance*.

This defines the distance between clusters as the distance between the two data points in
the clusters that are closest together.

For example, we would define the distance between the '*' and 'o' clusters by computing the 
Euclidean distance between these two points.

```
^
|           distance
|     *     /|
|   ****   / |  
|  **** * *  |      oooo
| *  *       o  ooo o
|             oooo
|
|
|    x
|     xxxx
|   x x x
|     x
+----------------------------->
```

The other points in the clusters could be really far away, but it doesn't matter if we use
minimum distance.

The only thing we care about is how close together the closest points are.

Alternatively, we could use *maximum distance*. This one computes the distance between the two
clusters as the distance between the two points that are the farthest apart.

So for example, we would compute the distance between the '*' and 'o' clusters
by looking at these two points.

```
^
|           distance min
|     *     /|
|   ****   / |  
|  **** * *  |      oooo
| *  *       o  ooo o  o
|   \          oooo   /
|    \_distance max__/
|
|    x
|     xxxx
|   x x x
|     x
+----------------------------->
```

Here, it doesn't matter how close together the other points are. All we care about is how 
close together the furthest points are.

The most common distance metric between clusters is called *centroid distance*.
And this is what we'll use.

It defines the distance between clusters by computing the centroid of the clusters.

The *centroid* is just the data point that takes the average of all data points in each 
component. This takes all data points in each cluster into account and can be thought of as 
the middle data point.

In our example, the centroids between '*' and 'o' are here, and we would compute the distance
between the clusters by computing the Euclidean distance between those two points.

```
^     distance centr
|        /    \
|     * /      \
|   ***/*       \
|  **** * *      \oooo
| *  *       o  ooo o
|             oooo
|
|
|    x
|     xxxx
|   x x x
|     x
+----------------------------->
```

When we are computing distances, it's highly influenced by the scale of the variables.

As an example, suppose you're computing the distance between two data points, where one
variable is the revenue of a company in thousands of dollars, and another is the age of the
company in years.

The revenue variable would really dominate in the distance calculation. The differences 
between the data points for revenue would be in the thousands. Whereas the differences between 
the year variable would probably be less than 10.

To handle this, it's customary to *normalize* the data first. 

We can normalize by subtracting the mean of the data and dividing by the standard deviation.

In our movie data set, all of our genre variables are on the same scale. So we don't have to worry 
about normalizing. But if we wanted to add a variable, like box office revenue, we would need 
to normalize so that this variable didn't dominate all of the others.

### Hierarchical Clustering

In hierarchical clustering, the clusters are formed by each data point starting in its own 
cluster.

As a small example, suppose we have five data points. Each data point is labeled as belonging 
in its own cluster. So this data point is in the red cluster, this one's in the blue cluster, 
this one's in the purple cluster, this one's in the green cluster, and this one's
in the yellow cluster.

Then hierarchical clustering combines the two nearest clusters into one cluster.

We'll use Euclidean and Centroid distances to decide which two clusters are the closest.

In our example, the green and yellow clusters are closest together.

So we would combine these two clusters into one cluster. So now the green cluster has two points,
and the yellow cluster is gone. 

Now this process repeats. We again find the two nearest clusters, which this time are the green
cluster and the purple cluster, and we combine them into one cluster. 

Now the green cluster has three points, and the purple cluster is gone.

Now the two nearest clusters are the red and blue clusters. So we would combine these two clusters
into one cluster, the red cluster. So now we have just two clusters, the red one and the green one.

So now the final step is to combine these two clusters into one cluster. 

So at the end of hierarchical clustering, all of our data points are in a single cluster.

The hierarchical cluster process can be displayed through what's called a *dendrogram*.

```
  4+            +---------------------+
   |            |                     |
   |            |                     |
  3+            |                     |
   |            |                     |
   |            |                     |
  2+            |                     |
   |            |                     |
   |            |                     |
  1+  +---------------+         +-----------+
   |  |               |         |           |
   |  3          +--------+     2           5
0.5+             |        |
                 1        4
```

The data points are listed along the bottom, and the lines show how the clusters were combined.

The height of the lines represents how far apart the clusters were when they were combined.

So points 1 and 4 were pretty close together when they were combined. But when we combined the 
two clusters at the end, they were significantly farther apart.

We can use a dendrogram to decide how many clusters we want for our final clustering model.

The easiest way to pick the number of clusters you want is to draw a horizontal line across 
the dendrogram. The number of vertical lines that line crosses is the number of clusters there
will be.

The farthest this horizontal line can move up and down in the dendrogram without hitting one
of the horizontal lines of the dendrogram, the better that choice of the number of clusters is.

But when picking the number of clusters, you should also consider how many clusters make sense 
for the particular application you're working with.

After selecting the number of clusters you want, you should analyze your clusters to see if
they're meaningful. This can be done by looking at basic statistics in each cluster, 
like the mean, maximum, and minimum values in each cluster and each variable.

You can also check to see if the clusters have a feature in common that was not used in the 
clustering, like an outcome variable. This often indicates that your clusters might help improve
a predictive model.

### Getting the Data

Let's switch to R and load our data, we'll be using a slightly different command this time
because the data is not a CSV file. It's a text file, where the entries are separated by a 
vertical bar.

```{r}
movies = read.table("movieLens.txt", header=FALSE, sep="|", quote = "\"")
# the last argument just made sure that our text is read in properly
str(movies)
```

We have 1,682 observations of 24 different variables. Since our variables didn't have names, 
header equaled false, R just labeled them with V1, V2, V3, etc. But from the Movie Lens documentation,
we know what these variables are.

```{r}
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
```

We can see that we have the same number of observations and the same number of variables,
but each of them now has the name that we just gave.

We won't be using the ID, release date, video release date, or IMDB variables, so let's go 
ahead and remove them.

```{r}
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
```

And there are a few duplicate entries in our data set, so we'll go ahead and remove them
with the unique function.

```{r}
movies = unique(movies)
str(movies)
```

Now, we have 1,664 observations, a few less than before, and 20 variables, the title of the movie, 
the unknown genre label, and then the 18 other genre labels.
